{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom timeouts applied: query=900 insert=300 init=120\n",
      "Server modules detected: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n",
      "Collection 'Dataset' ready (text2vec-transformers).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Sets up Weaviate collection using text2vec-transformers (external inference container).\n",
    "# Ensure docker-compose is up with services: weaviate + t2v-transformers.\n",
    "#   ENABLE_MODULES=text2vec-transformers,generative-ollama\n",
    "#   DEFAULT_VECTORIZER_MODULE=text2vec-transformers\n",
    "#   TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080\n",
    "# Optional: generative-ollama (Ollama running on host for qwen2m:latest)\n",
    "# Timeout tuning: Some weaviate client versions expose TimeoutConfig; if not, we fall back.\n",
    "\n",
    "# Increase default timeouts to reduce GRPC RST_STREAM errors during heavy queries\n",
    "CUSTOM_INIT_TIMEOUT = 120\n",
    "CUSTOM_QUERY_TIMEOUT = 900   # 15 minutes\n",
    "CUSTOM_INSERT_TIMEOUT = 300  # 5 minutes\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "from weaviate.config import AdditionalConfig, Timeout\n",
    "# Attempt optional TimeoutConfig (newer weaviate client). If missing, continue with defaults.\n",
    "try:\n",
    "    TIMEOUTS = Timeout(init=CUSTOM_INIT_TIMEOUT, query=CUSTOM_QUERY_TIMEOUT, insert=CUSTOM_INSERT_TIMEOUT)\n",
    "    client = weaviate.connect_to_local(\n",
    "        additional_config=AdditionalConfig(timeout=TIMEOUTS))\n",
    "    print(\"Custom timeouts applied:\", TIMEOUTS)\n",
    "except ImportError:\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig not available in this weaviate version; using default client timeouts.\")\n",
    "    print(\"Tip: pip install --upgrade weaviate-client to enable configurable timeouts.\")\n",
    "except TypeError:\n",
    "    # Signature mismatch (older version). Reconnect without custom timeouts.\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig signature unsupported; using default timeouts.\")\n",
    "    print(\"Tip: pip install --upgrade weaviate-client to enable configurable timeouts.\")\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # must match the model available to Ollama on host\n",
    "\n",
    "# Diagnostics\n",
    "try:\n",
    "    meta = client.get_meta()\n",
    "    print(\"Server modules detected:\", list(meta.get(\"modules\", {}).keys()))\n",
    "except Exception as e:\n",
    "    print(\"Meta fetch failed:\", e)\n",
    "\n",
    "# Recreate collection for a clean slate\n",
    "try:\n",
    "    client.collections.delete(\"Dataset\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "api_endpoint = \"http://host.docker.internal:11434\"  # Ollama on host\n",
    "\n",
    "client.collections.create(\n",
    "    \"Dataset\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.LOWERCASE),\n",
    "        Property(name=\"file_path\", data_type=DataType.TEXT)\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_transformers(\n",
    "            name=\"text_vector\",\n",
    "            source_properties=[\"text\"],\n",
    "            pooling_strategy=\"masked_mean\",\n",
    "        )\n",
    "    ],\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=api_endpoint,\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "assert client.collections.exists(\"Dataset\")\n",
    "print(\"Collection 'Dataset' ready (text2vec-transformers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3a158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\warnings.py:302: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Francisco Azeredo\\AppData\\Local\\Temp\\ipykernel_38388\\2820299804.py:116: ResourceWarning: unclosed <socket.socket fd=6132, family=23, type=1, proto=0, laddr=('::1', 54990, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  rag = client.collections.get(\"Dataset\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08426e6cb467407bbaa9759d8bf92762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 442 docs in 0.07s\n",
      "Start RSS: 811.84 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e95cd70df64a338d0e828865d65d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 442 / 442 docs in 6.29s (70.22 docs/s)\n",
      "End RSS: 812.27 MB (Œî 0.43 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\C√≥digo\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\C√≥digo\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    from summarization.lexrank_summarizer import summarize_text_lexrank\n",
    "    from summarization.bart_summarizer import summarize_text_bart\n",
    "    for p in tqdm(paths, desc=\"Loading documents\", total=len(paths)):\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        # print(f\"======Before summarization======\")\n",
    "        # print(f\"{text}\")\n",
    "        # text = summarize_text_lexrank(text, ratio=1.0)\n",
    "        # text = \" \".join(text)\n",
    "        # print(f\"{text}\")\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Œî {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(\"Dataset\")\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbd498f9196450fbd67d9cb7ded8d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 docs in 6.07s\n",
      "Start RSS: 950.39 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7c41f75eee452197ca27fc56c840a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 / 1000 docs in 9.43s (106.05 docs/s)\n",
      "End RSS: 871.78 MB (Œî -78.62 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\documentos_gerados\"\n",
    "WORKING_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "        if suffix == \".docx\":\n",
    "            # Read .docx files using python-docx\n",
    "            doc = Document(path)\n",
    "            text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    text.append(paragraph.text.strip())\n",
    "            return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\", \".docx\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    from summarization.lexrank_summarizer import summarize_text_lexrank\n",
    "    from summarization.bart_summarizer import summarize_text_bart\n",
    "    for p in tqdm(paths, desc=\"Loading documents\", total=len(paths)):\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        # text = summarize_text_lexrank(text, ratio=1.0)\n",
    "        # text = \" \".join(text)\n",
    "\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Œî {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(\"Dataset\")\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82e903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Warming up Ollama model: qwen2.5:latest\n",
      "‚Ä¶waiting for Ollama to load model (poll 5s). Last error: HTTP 500: {\"error\":\"timed out waiting for llama runner to start - progress 0.00 - \"}\n",
      "‚úÖ Ollama ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Warm up Ollama (Optional)\n",
    "# ----------------------------------\n",
    "# Ensures the Ollama model is loaded before Weaviate calls it.\n",
    "\n",
    "# You can rerun this cell anytime after starting Ollama or changing models.\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configure warm-up behavior\n",
    "AUTO_WARMUP_OLLAMA = True\n",
    "WARMUP_TIMEOUT = 600          # seconds to wait for model to be ready\n",
    "WARMUP_POLL_INTERVAL = 5      # seconds between checks\n",
    "WARMUP_API_URL = \"http://localhost:11434/api/generate\"  # Ollama generate endpoint\n",
    "\n",
    "def warm_up_ollama(model: str | None = None, timeout_s: int = WARMUP_TIMEOUT, poll_interval: int = WARMUP_POLL_INTERVAL, prompt: str = \"ping\", api_url: str = WARMUP_API_URL) -> bool:\n",
    "    \"\"\"Ping Ollama generate API until the model is ready or timeout expires.\n",
    "    Returns True when ready, False if timed out.\"\"\"\n",
    "    mdl = model or globals().get(\"LLM_MODEL_NAME\", \"qwen2.5:latest\")\n",
    "    print(f\"üîß Warming up Ollama model: {mdl}\")\n",
    "    start = time.perf_counter()\n",
    "    last_err = None\n",
    "    while (time.perf_counter() - start) < timeout_s:\n",
    "        try:\n",
    "            resp = requests.post(api_url, json={\"model\": mdl, \"prompt\": prompt, \"stream\": False}, timeout=3000)\n",
    "            if resp.status_code == 200:\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "                text = data.get(\"response\") or data.get(\"message\") or \"\"\n",
    "                print(\"‚úÖ Ollama ready.\")\n",
    "                return True\n",
    "            else:\n",
    "                last_err = f\"HTTP {resp.status_code}: {resp.text[:200]}\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "        print(f\"‚Ä¶waiting for Ollama to load model (poll {poll_interval}s). Last error: {last_err}\")\n",
    "        time.sleep(poll_interval)\n",
    "    print(f\"‚è±Ô∏è Warm-up timed out after {timeout_s}s. Last error: {last_err}\")\n",
    "    return False\n",
    "\n",
    "# Auto warm-up when this cell runs\n",
    "if AUTO_WARMUP_OLLAMA:\n",
    "    warm_up_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 358 QA pairs from JSON.\n",
      "üîß Warming up Ollama model: qwen2.5:latest\n",
      "‚úÖ Ollama ready.\n",
      "üöÄ Starting RAG evaluation com sa√≠da garantida em portugu√™s...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7893854901e4b0d91eea8dda7389e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-best_metrics:   0%|          | 0/358 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: Segundo a comunica√ß√£o de 15/10/2023 da Direc√ß√£o-Geral da Administra√ß√£o da Justi√ßa, qual foi a norma ...\n",
      "Generated: De acordo com a comunica√ß√£o da Dire√ß√£o-Geral da Administra√ß√£o da Justi√ßa, a norma declarada inconstitucional foi o artig\n",
      "Expected: Foi declarada inconstitucional a norma do artigo 45.¬∫, n.¬∫ 2, da Lei n.¬∫ 45/2020, de 30 de Junho. A decis√£o baseou-se no\n",
      "Expected File: documento_115_100.10.003.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_535_100.10.003.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.515', 'rouge1_f': '0.250', 'bert_cos': '0.761'}\n",
      "Latency: 14943.3ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q2: No documento de 15 de outubro de 2023, qual √© a data do parecer do MNE e qual a sua conclus√£o sobre ...\n",
      "Generated: O documento n√£o cont√©m informa√ß√µes sobre pareceres do MNE, encargos financeiros relevantes para a data especificada (15 \n",
      "Expected: O parecer do MNE √© datado de 5 de outubro de 2023 e conclui que a ades√£o n√£o gera encargos financeiros significativos. O\n",
      "Expected File: documento_686_100.1003.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_324_100.10.02.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.367', 'rouge1_f': '0.310', 'bert_cos': '0.767'}\n",
      "Latency: 8939.6ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q3: Segundo o despacho deste documento, onde o Regulamento Interno do Centro Cultural de Exemplo deve se...\n",
      "Generated: O Regulamento Interno do Centro Cultural Municipal deve ser publicado no Di√°rio Oficial do Munic√≠pio e divulgado nos can\n",
      "Expected: Deve ser publicado no Di√°rio da C√¢mara Municipal e divulgado em todos os meios dispon√≠veis, com o objetivo de garantir o\n",
      "Expected File: documento_423_100.APR.asdf.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_207_100.APR.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.654', 'rouge1_f': '0.375', 'bert_cos': '0.699'}\n",
      "Latency: 15319.1ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "RAG EVALUATION RESULTS - BEST METRICS ONLY\n",
      "============================================================\n",
      "\n",
      "üéØ RETRIEVAL PERFORMANCE:\n",
      "  Document Retrieval Accuracy: 103/358 = 28.77%\n",
      "  Average Retrieval Rank: -0.7\n",
      "\n",
      "üìù ANSWER QUALITY:\n",
      "  Exact Match: 0.00%\n",
      "  Substring Match: 5.59%\n",
      "  Token Recall: 0.382\n",
      "  ROUGE-1 F1: 0.205\n",
      "  BERT Similarity: 0.488\n",
      "\n",
      "üîç CONTEXT QUALITY:\n",
      "\n",
      "‚ö° PERFORMANCE:\n",
      "  Average Latency: 12097.5ms\n",
      "  95th Percentile Latency: 19440.6ms\n",
      "  Questions per Second: 0.08\n",
      "\n",
      "üíæ Results saved to: C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\best_results_best_metrics306.csv\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "\n",
      "‚úÖ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation - Best Metrics Only\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Optimized to focus on the most meaningful metrics for RAG evaluation.\n",
    "\n",
    "from pickle import FALSE\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from regex import F\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration - Using Relative Paths --------\n",
    "# Updated to use relative paths after reorganization\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "QA_JSON_PATH = str(PROJECT_ROOT / \"datasets\" / \"merged_qa_dataset.json\")\n",
    "OUTPUT_CSV_PATH = str(PROJECT_ROOT / \"results_csv\")  # set to None to skip saving\n",
    "TOP_K = 3            # lower to reduce vector fetch time\n",
    "MAX_Q = None         # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True  # semantic metrics cost\n",
    "PER_QUERY_DEADLINE = 60000.0  # seconds, must be < client.query timeout\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF = 10.0  # seconds added each retry\n",
    "LANGUAGE = \"pt-BR\"\n",
    "PROMPT_PREFIX = \"Responda de forma breve e objetiva em portugu√™s (pt-BR): \"\n",
    "COT_PROMPT = True  # chain-of-thought prompting (slower, may improve complex Qs)\n",
    "AUTO_WARMUP_OLLAMA = globals().get('AUTO_WARMUP_OLLAMA', True)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"‚úì Paths configured:\")\n",
    "print(f\"  Q&A Dataset: {QA_JSON_PATH}\")\n",
    "print(f\"  Output Folder: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  Dataset exists: {Path(QA_JSON_PATH).exists()}\")\n",
    "print(f\"  Output folder exists: {Path(OUTPUT_CSV_PATH).exists()}\")\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        print(\"Loading BERT model (all-MiniLM-L6-v2)...\")\n",
    "        _BERT_MODEL = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def tokenize_pt(s: str):\n",
    "    return [t.lower() for t in TOKEN_SPLIT_RE.split(s) if t.strip()]\n",
    "\n",
    "def token_recall(answer: str, gold: str) -> float:\n",
    "    at = set(tokenize_pt(answer))\n",
    "    gt = set(tokenize_pt(gold))\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    return len(at & gt) / len(gt)\n",
    "\n",
    "def compute_rouge1(hyp: str, ref: str):\n",
    "    if not hyp.strip() or not ref.strip():\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "    try:\n",
    "        return _lazy_rouge().get_scores(hyp, ref)[0]\n",
    "    except:\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "\n",
    "def compute_bleu(hyp: str, ref: str) -> float:\n",
    "    h_toks = tokenize_pt(hyp)\n",
    "    r_toks = tokenize_pt(ref)\n",
    "    if not h_toks or not r_toks:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return sentence_bleu([r_toks], h_toks, smoothing_function=_SMOOTH)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_bert_similarity(hyp: str, ref: str) -> float:\n",
    "    if not USE_BERT_SIM or not hyp.strip() or not ref.strip():\n",
    "        return 0.0\n",
    "    model = _lazy_bert()\n",
    "    emb = model.encode([hyp, ref])\n",
    "    return float(np.dot(emb[0], emb[1]) / (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1])))\n",
    "\n",
    "def best_sentence_overlap(answer: str, gold: str) -> float:\n",
    "    sents_a = sent_tokenize(answer, language=\"portuguese\")\n",
    "    sents_g = sent_tokenize(gold, language=\"portuguese\")\n",
    "    best = 0.0\n",
    "    for sa in sents_a:\n",
    "        for sg in sents_g:\n",
    "            tr = token_recall(sa, sg)\n",
    "            if tr > best:\n",
    "                best = tr\n",
    "    return best\n",
    "\n",
    "# Load QA\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_JSON_PATH):\n",
    "    with open(QA_JSON_PATH, encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "        for item in qa_data:\n",
    "            if \"pergunta\" in item and \"resposta\" in item:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": item[\"pergunta\"].strip(),\n",
    "                    \"gold_answer\": item[\"resposta\"].strip(),\n",
    "                    \"context\": item.get(\"contexto\",\"\").strip(),\n",
    "                    \"file\": item.get(\"arquivo\",\"\").strip()\n",
    "                })\n",
    "else:\n",
    "    print(\"QA JSON not found. Check QA_JSON_PATH.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    random.shuffle(qa_pairs)\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} Q&A pairs.\")\n",
    "\n",
    "# Optional warmup\n",
    "if AUTO_WARMUP_OLLAMA:\n",
    "    warm_up_ollama()\n",
    "\n",
    "rows = []\n",
    "latencies = []\n",
    "correct_retrievals = 0\n",
    "cot_phrase = \" Vamos pensar passo a passo.\" if COT_PROMPT else \"\"\n",
    "\n",
    "for idx, pair in enumerate(tqdm(qa_pairs, desc=\"Evaluating (Best Metrics)\")):\n",
    "    q = pair[\"question\"]\n",
    "    gold_answer = pair[\"gold_answer\"]\n",
    "    gold_context = pair[\"context\"]\n",
    "    gold_file = pair[\"file\"]\n",
    "    \n",
    "    start_t = time.perf_counter()\n",
    "    try:\n",
    "        response_obj = rag.query(q + cot_phrase, param={\"limit\": TOP_K})\n",
    "        \n",
    "        if not response_obj or not hasattr(response_obj, \"answer\"):\n",
    "            print(f\"Q#{idx+1}: No valid response. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        answer_text = response_obj.answer.strip() if response_obj.answer else \"\"\n",
    "        retrieved_context = \" \".join([ctx.get(\"content\", \"\") for ctx in response_obj.context]) if hasattr(response_obj, \"context\") else \"\"\n",
    "        \n",
    "        # Retrieval metrics\n",
    "        retrieved_files = [ctx.get(\"file_path\", \"\") for ctx in response_obj.context] if hasattr(response_obj, \"context\") else []\n",
    "        rank = retrieved_files.index(gold_file) if gold_file in retrieved_files else -1\n",
    "        if rank >= 0:\n",
    "            correct_retrievals += 1\n",
    "        \n",
    "        # Answer quality (best metrics)\n",
    "        exact = 1.0 if answer_text.lower() == gold_answer.lower() else 0.0\n",
    "        substring = 1.0 if gold_answer.lower() in answer_text.lower() else 0.0\n",
    "        tok_recall = token_recall(answer_text, gold_answer)\n",
    "        rouge1_scores = compute_rouge1(answer_text, gold_answer)\n",
    "        rouge1_f = rouge1_scores[\"rouge-1\"][\"f\"]\n",
    "        bert_cos = compute_bert_similarity(answer_text, gold_answer) if USE_BERT_SIM else 0.0\n",
    "        \n",
    "        # Context quality\n",
    "        context_token_recall = token_recall(retrieved_context, gold_context) if gold_context else 0.0\n",
    "        context_rouge1 = compute_rouge1(retrieved_context, gold_context) if gold_context else {\"rouge-1\": {\"f\": 0.0}}\n",
    "        context_bert = compute_bert_similarity(retrieved_context, gold_context) if USE_BERT_SIM and gold_context else 0.0\n",
    "        \n",
    "        lat = time.perf_counter() - start_t\n",
    "        latencies.append(lat)\n",
    "        \n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"generated_answer\": answer_text,\n",
    "            \"gold_file\": gold_file,\n",
    "            \"retrieved_files\": \"|\".join(retrieved_files),\n",
    "            \"retrieval_rank\": rank,\n",
    "            \"exact\": exact,\n",
    "            \"substring\": substring,\n",
    "            \"token_recall\": tok_recall,\n",
    "            \"rouge1_f\": rouge1_f,\n",
    "            \"bert_cos\": bert_cos,\n",
    "            \"context_token_recall\": context_token_recall,\n",
    "            \"context_rouge1_f\": context_rouge1[\"rouge-1\"][\"f\"],\n",
    "            \"context_bert_cos\": context_bert,\n",
    "            \"latency_s\": lat\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Q#{idx+1} error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Results summary\n",
    "if rows:\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    \n",
    "    # Report best metrics only\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAG EVALUATION RESULTS - BEST METRICS ONLY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RETRIEVAL PERFORMANCE:\")\n",
    "    print(f\"  Document Retrieval Accuracy: {correct_retrievals}/{len(qa_pairs)} = {correct_retrievals/len(qa_pairs):.2%}\")\n",
    "    print(f\"  Average Retrieval Rank: {_avg('retrieval_rank'):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìù ANSWER QUALITY:\")\n",
    "    print(f\"  Exact Match: {_avg('exact'):.2%}\")\n",
    "    print(f\"  Substring Match: {_avg('substring'):.2%}\")\n",
    "    print(f\"  Token Recall: {_avg('token_recall'):.3f}\")\n",
    "    print(f\"  ROUGE-1 F1: {_avg('rouge1_f'):.3f}\")\n",
    "    if 'bert_cos' in rows[0]:\n",
    "        print(f\"  BERT Similarity: {_avg('bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüîç CONTEXT QUALITY:\")\n",
    "    if 'context_token_recall' in rows[0]:\n",
    "        print(f\"  Context Token Recall: {_avg('context_token_recall'):.3f}\")\n",
    "    if 'context_rouge1_f' in rows[0]:\n",
    "        print(f\"  Context ROUGE-1 F1: {_avg('context_rouge1_f'):.3f}\")\n",
    "    if 'context_bert_cos' in rows[0]:\n",
    "        print(f\"  Context BERT Similarity: {_avg('context_bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95 = statistics.quantiles(latencies, n=20)[18] if len(latencies)>=20 else max(latencies)\n",
    "    print(f\"  Average Latency: {avg_lat*1000:.1f}ms\")\n",
    "    print(f\"  95th Percentile Latency: {p95*1000:.1f}ms\")\n",
    "    print(f\"  Questions per Second: {1/avg_lat:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        mode = \"CoT\" if COT_PROMPT else \"Naive\"\n",
    "        n = len(rows)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"best_results_{mode}{n}.csv\")\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"\\n‚úì Results saved to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo valid results to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76652161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing connections...\n",
      "‚úÖ Weaviate client connected\n",
      "‚úÖ Ollama is running with 5 models\n",
      "‚úÖ qwen2.5 model is available\n",
      "‚úÖ Weaviate is running, modules: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n",
      "‚úÖ Dataset collection has 1442 documents\n",
      "‚úÖ Vector search is working\n",
      "‚úÖ Generative search is working and responding in Portuguese\n",
      "\n",
      "üéØ If all tests pass, you can proceed with the benchmark.\n",
      "üéØ If tests fail, fix the issues before running the evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Test connections before running benchmark\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"üîç Testing connections...\")\n",
    "import weaviate\n",
    "from weaviate.config import AdditionalConfig, Timeout\n",
    "# Connect to Weaviate with custom timeouts\n",
    "\n",
    "CUSTOM_INIT_TIMEOUT = 120\n",
    "CUSTOM_QUERY_TIMEOUT = 900   # 15 minutes\n",
    "CUSTOM_INSERT_TIMEOUT = 300  # 5 minutes\n",
    "try:\n",
    "    TIMEOUTS = Timeout(init=CUSTOM_INIT_TIMEOUT, query=CUSTOM_QUERY_TIMEOUT, insert=CUSTOM_INSERT_TIMEOUT)\n",
    "    client = weaviate.connect_to_local(additional_config=AdditionalConfig(timeout=TIMEOUTS))\n",
    "    print(\"‚úÖ Weaviate client connected\")\n",
    "except Exception as e:\n",
    "    print(\"Weaviate timeout config error:\", e)\n",
    "# Test Ollama\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(f\"‚úÖ Ollama is running with {len(models)} models\")\n",
    "        qwen_available = any('qwen2.5' in model.get('name', '') for model in models)\n",
    "        if qwen_available:\n",
    "            print(\"‚úÖ qwen2.5 model is available\")\n",
    "        else:\n",
    "            print(\"‚ùå qwen2.5 model not found - run: ollama pull qwen2.5:latest\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama responded with status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection failed: {e}\")\n",
    "    print(\"Run: ollama serve\")\n",
    "\n",
    "# Test Weaviate\n",
    "try:\n",
    "    if 'client' in globals():\n",
    "        meta = client.get_meta()\n",
    "        print(f\"‚úÖ Weaviate is running, modules: {list(meta.get('modules', {}).keys())}\")\n",
    "        \n",
    "        # Test collection exists\n",
    "        if client.collections.exists(\"Dataset\"):\n",
    "            collection = client.collections.get(\"Dataset\")\n",
    "            aggregate = collection.aggregate.over_all(total_count=True)\n",
    "            count = aggregate.total_count\n",
    "            print(f\"‚úÖ Dataset collection has {count} documents\")\n",
    "            \n",
    "            # Simple test query (no generation)\n",
    "            try:\n",
    "                test_result = collection.query.near_text(\n",
    "                    query=\"teste\", \n",
    "                    limit=1,\n",
    "                    return_metadata=['distance']\n",
    "                )\n",
    "                print(\"‚úÖ Vector search is working\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Vector search failed: {e}\")\n",
    "            # Simple test generation (Portuguese enforced)\n",
    "            try:\n",
    "                gen_result = collection.generate.hybrid(\n",
    "                    query=\"O que √© IA?\", \n",
    "                    limit=1,\n",
    "                    target_vector=\"text_vector\",\n",
    "                    grouped_task=\"Explique brevemente em portugu√™s (pt-BR): O que √© IA?\",\n",
    "                    return_metadata=['distance']\n",
    "                )\n",
    "                gen_text = getattr(getattr(gen_result, 'generative', None), 'text', None)\n",
    "                if gen_text:\n",
    "                    if any('\\u4e00' <= ch <= '\\u9fff' for ch in gen_text):\n",
    "                        print(\"‚ö†Ô∏è Generative search returned Chinese text; prompts will enforce pt-BR in the evaluation cell.\")\n",
    "                    else:\n",
    "                        print(\"‚úÖ Generative search is working and responding in Portuguese\")\n",
    "                else:\n",
    "                    print(\"‚ùå Generative search returned no text\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generative search failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå Dataset collection not found\")\n",
    "    else:\n",
    "        print(\"‚ùå Weaviate client not initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Weaviate connection failed: {e}\")\n",
    "\n",
    "print(\"\\nüéØ If all tests pass, you can proceed with the benchmark.\")\n",
    "print(\"üéØ If tests fail, fix the issues before running the evaluation.\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
