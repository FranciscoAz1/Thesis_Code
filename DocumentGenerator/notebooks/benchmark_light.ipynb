{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Load (26366, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_entities.json'} 26366 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_entities.json'} 26366 data\n",
      "INFO:nano-vectordb:Load (26366, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_entities_name.json'} 26366 data\n",
      "INFO:nano-vectordb:Load (26366, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_entities_name.json'} 26366 data\n",
      "INFO:nano-vectordb:Load (27883, 384) data\n",
      "INFO:nano-vectordb:Load (27883, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_relationships.json'} 27883 data\n",
      "INFO:nano-vectordb:Load (474, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_chunks.json'} 474 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_relationships.json'} 27883 data\n",
      "INFO:nano-vectordb:Load (474, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\storage\\\\vdb_chunks.json'} 474 data\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "import os, torch, sys\n",
    "import minirag\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag.llm.openai import openai_complete, openai_queue_completion\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # must match the model available to Ollama on host\n",
    "WORKING_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\storage\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func = lambda texts: hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model),\n",
    ")\n",
    "rag = minirag.MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "    llm_model_name=LLM_MODEL_NAME,\n",
    "    embedding_func=_embedding_func,\n",
    "    log_level=LOG_LEVEL,\n",
    "    suppress_httpx_logging=True,\n",
    "    entity_presidio_extraction=False,\n",
    ")\n",
    "# rag = minirag.MiniRAG(\n",
    "#     working_dir=WORKING_DIR,\n",
    "#     llm_model_func=openai_queue_completion,\n",
    "#     llm_model_max_token_size=200,\n",
    "#     llm_model_kwargs={\"api_key\": api_key},\n",
    "#     # llm_model_name=LLM_MODEL_NAME,\n",
    "#     llm_model_name=\"gpt-5-nano\",\n",
    "#     embedding_func=_embedding_func,\n",
    "#     log_level=LOG_LEVEL,\n",
    "#     suppress_httpx_logging=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 20 docs in 0.12s\n",
      "Start RSS: 1030.40 MB\n",
      "Indexing with ainsert() ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80434fa53e0d4df380d98dd878db6612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 20 docs in 43326.59s (0.00 docs/s)cated), 850 relations(duplicated)\n",
      "End RSS: 891.28 MB (Œî -139.12 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\curated_docs\"\n",
    "WORKING_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "        if suffix == \".docx\":\n",
    "            # Read .docx files using python-docx\n",
    "            doc = Document(path)\n",
    "            text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    text.append(paragraph.text.strip())\n",
    "            return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\", \".docx\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing with ainsert() ...\")\n",
    "    t1 = time.perf_counter()\n",
    "    for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "        try:\n",
    "            await rag.ainsert(text, metadata=metadata, file_path=metadata.get(\"source\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)} docs in {dur:.2f}s ({len(texts)/dur:.2f} docs/s)\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Œî {end_mem - start_mem:.2f} MB)\")\n",
    "global rag\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 QA pairs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bc8d92c2e54a8e9353b1593023d58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-mini:   0%|          | 0/20 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Quais s√£o os principais pontos abordados na proposta de lei sobre a prote√ß√£o de ...\n",
      "Answer: Infelizmente, n√£o h√° informa√ß√µes dispon√≠veis nas tabelas fornecidas que possam ser usadas diretamente para responder √† sua pergunta sobre a proposta de lei sobr\n",
      "Gold: A proposta de lei inclui os seguintes pontos principais: 1. Amplia√ß√£o da defini√ß√£o e categoriza√ß√£o dos dados pessoais, incluindo novas formas de tratamento que \n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.588', 'jaccard': '0.148', 'levenshtein': '0.094', 'rouge1_f': '0.253', 'rouge2_f': '0.095', 'overlap': '1.000', 'bleu': '0.021', 'bert_cos': '0.768'} Latency: 257134.6 ms attempts=1\n",
      "-\n",
      "Q2: Qual √© o prazo estabelecido para a conclus√£o da consulta p√∫blica e entrega da ve...\n",
      "Answer: Infelizmente, as informa√ß√µes fornecidas em suas tabelas n√£o cont√™m detalhes espec√≠ficos sobre prazos ou cronogramas para a conclus√£o da consulta p√∫blica e entre\n",
      "Gold: O prazo para a conclus√£o da consulta p√∫blica e entrega da vers√£o final do documento normativo ser√° de seis meses a contar da data do despacho, que √© 15 de Outub\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.593', 'jaccard': '0.298', 'levenshtein': '0.211', 'rouge1_f': '0.459', 'rouge2_f': '0.377', 'overlap': '0.519', 'bleu': '0.332', 'bert_cos': '0.654'} Latency: 10916.8 ms attempts=1\n",
      "-\n",
      "Q2: Qual √© o prazo estabelecido para a conclus√£o da consulta p√∫blica e entrega da ve...\n",
      "Answer: Infelizmente, as informa√ß√µes fornecidas em suas tabelas n√£o cont√™m detalhes espec√≠ficos sobre prazos ou cronogramas para a conclus√£o da consulta p√∫blica e entre\n",
      "Gold: O prazo para a conclus√£o da consulta p√∫blica e entrega da vers√£o final do documento normativo ser√° de seis meses a contar da data do despacho, que √© 15 de Outub\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.593', 'jaccard': '0.298', 'levenshtein': '0.211', 'rouge1_f': '0.459', 'rouge2_f': '0.377', 'overlap': '0.519', 'bleu': '0.332', 'bert_cos': '0.654'} Latency: 10916.8 ms attempts=1\n",
      "-\n",
      "\n",
      "Aggregate: exact=0.00% substring=0.00% token_recall=52.91%\n",
      "  jaccard: 0.190\n",
      "  levenshtein: 0.142\n",
      "  rouge1_f: 0.301\n",
      "  rouge2_f: 0.145\n",
      "  overlap: 0.494\n",
      "  bleu: 0.096\n",
      "  bert_cos: 0.704\n",
      "Latency: avg=26258.9 ms p95=54928.7 ms\n",
      "Saved results to results\\results_mini8.csv\n",
      "Evaluation complete.\n",
      "\n",
      "Aggregate: exact=0.00% substring=0.00% token_recall=52.91%\n",
      "  jaccard: 0.190\n",
      "  levenshtein: 0.142\n",
      "  rouge1_f: 0.301\n",
      "  rouge2_f: 0.145\n",
      "  overlap: 0.494\n",
      "  bleu: 0.096\n",
      "  bert_cos: 0.704\n",
      "Latency: avg=26258.9 ms p95=54928.7 ms\n",
      "Saved results to results\\results_mini8.csv\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Timeout mitigation strategies applied: reduced limit, concise prompts, retries with backoff.\n",
    "\n",
    "# ----------- Constants & Config - Using Relative Paths --------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Updated to use relative paths after reorganization\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "QA_JSON_PATH = str(PROJECT_ROOT / \"datasets\" / \"qa_dataset.json\")  # Path to QA dataset (JSON)\n",
    "OUTPUT_CSV_PATH = str(PROJECT_ROOT / \"results_csv\")                # Output directory for CSV results\n",
    "MAX_Q = None                       # Limit number of QA pairs (None = all)\n",
    "TOP_K = 3                          # Top-K docs to retrieve per query\n",
    "MAX_RETRIES = 2                    # Max retries per query\n",
    "PER_QUERY_DEADLINE = 30            # Max seconds per query\n",
    "TOKEN_LIMIT = 512                  # Token limit for truncation (if used)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"‚úì Paths configured:\")\n",
    "print(f\"  Q&A Dataset: {QA_JSON_PATH}\")\n",
    "print(f\"  Output Folder: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  Dataset exists: {Path(QA_JSON_PATH).exists()}\")\n",
    "print(f\"  Output folder exists: {Path(OUTPUT_CSV_PATH).exists()}\")\n",
    "# --------------------------------------------\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from uu import Error\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) ‚Äì not used now\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def tokenize_pt(s: str):\n",
    "    return [t.lower() for t in TOKEN_SPLIT_RE.split(s) if t.strip()]\n",
    "\n",
    "def token_recall(answer: str, gold: str) -> float:\n",
    "    at = set(tokenize_pt(answer))\n",
    "    gt = set(tokenize_pt(gold))\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    return len(at & gt) / len(gt)\n",
    "\n",
    "def jaccard_similarity(answer: str, gold: str) -> float:\n",
    "    at = set(tokenize_pt(answer))\n",
    "    gt = set(tokenize_pt(gold))\n",
    "    union = at | gt\n",
    "    intersection = at & gt\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def compute_rouge1(hyp: str, ref: str):\n",
    "    if not hyp.strip() or not ref.strip():\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "    try:\n",
    "        return _lazy_rouge().get_scores(hyp, ref)[0]\n",
    "    except:\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "\n",
    "def compute_bleu(hyp: str, ref: str) -> float:\n",
    "    h_toks = tokenize_pt(hyp)\n",
    "    r_toks = tokenize_pt(ref)\n",
    "    if not h_toks or not r_toks:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return sentence_bleu([r_toks], h_toks, smoothing_function=_SMOOTH)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_bert_similarity(hyp: str, ref: str) -> float:\n",
    "    if not hyp.strip() or not ref.strip():\n",
    "        return 0.0\n",
    "    model = _lazy_bert()\n",
    "    emb = model.encode([hyp, ref])\n",
    "    return float(np.dot(emb[0], emb[1]) / (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1])))\n",
    "\n",
    "def best_sentence_overlap(answer: str, gold: str) -> float:\n",
    "    sents_a = sent_tokenize(answer, language=\"portuguese\")\n",
    "    sents_g = sent_tokenize(gold, language=\"portuguese\")\n",
    "    best = 0.0\n",
    "    for sa in sents_a:\n",
    "        for sg in sents_g:\n",
    "            tr = token_recall(sa, sg)\n",
    "            if tr > best:\n",
    "                best = tr\n",
    "    return best\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "# Load QA pairs from JSON\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_JSON_PATH):\n",
    "    try:\n",
    "        with open(QA_JSON_PATH, encoding=\"utf-8\") as f:\n",
    "            qa_data = json.load(f)\n",
    "            for item in qa_data:\n",
    "                if \"pergunta\" in item and \"resposta\" in item:\n",
    "                    qa_pairs.append({\n",
    "                        \"question\": item[\"pergunta\"].strip(),\n",
    "                        \"gold_answer\": item[\"resposta\"].strip(),\n",
    "                        \"context\": item.get(\"contexto\",\"\").strip(),\n",
    "                        \"file\": item.get(\"arquivo\",\"\").strip()\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading QA dataset: {e}\")\n",
    "else:\n",
    "    print(\"QA dataset not found. Provide QA_JSON_PATH or ensure qa_dataset.json is present.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    random.shuffle(qa_pairs)\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} Q&A pairs.\")\n",
    "\n",
    "# Evaluation loop\n",
    "rows = []\n",
    "latencies = []\n",
    "correct_retrievals = 0\n",
    "\n",
    "for idx, pair in enumerate(tqdm(qa_pairs, desc=\"Evaluating\")):\n",
    "    q = pair[\"question\"]\n",
    "    gold_answer = pair[\"gold_answer\"]\n",
    "    gold_context = pair[\"context\"]\n",
    "    gold_file = pair[\"file\"]\n",
    "    \n",
    "    start_t = time.perf_counter()\n",
    "    \n",
    "    for retry in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Query with timeout\n",
    "            async def query_with_timeout():\n",
    "                return await asyncio.wait_for(\n",
    "                    rag.aquery(q, param=QueryParam(top_k=TOP_K)),\n",
    "                    timeout=PER_QUERY_DEADLINE\n",
    "                )\n",
    "            \n",
    "            response_obj = await query_with_timeout()\n",
    "            \n",
    "            if not response_obj or not hasattr(response_obj, \"answer\"):\n",
    "                print(f\"Q#{idx+1}: No valid response. Skipping.\")\n",
    "                break\n",
    "            \n",
    "            answer_text = response_obj.answer.strip() if response_obj.answer else \"\"\n",
    "            retrieved_context = \" \".join([ctx.get(\"content\", \"\") for ctx in response_obj.context]) if hasattr(response_obj, \"context\") else \"\"\n",
    "            \n",
    "            # Retrieval metrics\n",
    "            retrieved_files = [ctx.get(\"file_path\", \"\") for ctx in response_obj.context] if hasattr(response_obj, \"context\") else []\n",
    "            rank = retrieved_files.index(gold_file) if gold_file in retrieved_files else -1\n",
    "            if rank >= 0:\n",
    "                correct_retrievals += 1\n",
    "            \n",
    "            # Answer quality metrics\n",
    "            exact = 1.0 if normalize_text(answer_text) == normalize_text(gold_answer) else 0.0\n",
    "            substring = 1.0 if gold_answer.lower() in answer_text.lower() else 0.0\n",
    "            tok_recall = token_recall(answer_text, gold_answer)\n",
    "            jaccard = jaccard_similarity(answer_text, gold_answer)\n",
    "            rouge1_scores = compute_rouge1(answer_text, gold_answer)\n",
    "            rouge1_f = rouge1_scores[\"rouge-1\"][\"f\"]\n",
    "            bleu_score = compute_bleu(answer_text, gold_answer)\n",
    "            bert_cos = compute_bert_similarity(answer_text, gold_answer)\n",
    "            \n",
    "            lat = time.perf_counter() - start_t\n",
    "            latencies.append(lat)\n",
    "            \n",
    "            rows.append({\n",
    "                \"question\": q,\n",
    "                \"gold_answer\": gold_answer,\n",
    "                \"generated_answer\": answer_text,\n",
    "                \"gold_file\": gold_file,\n",
    "                \"retrieved_files\": \"|\".join(retrieved_files),\n",
    "                \"retrieval_rank\": rank,\n",
    "                \"exact\": exact,\n",
    "                \"substring\": substring,\n",
    "                \"token_recall\": tok_recall,\n",
    "                \"jaccard\": jaccard,\n",
    "                \"rouge1_f\": rouge1_f,\n",
    "                \"bleu\": bleu_score,\n",
    "                \"bert_cos\": bert_cos,\n",
    "                \"latency_s\": lat\n",
    "            })\n",
    "            break\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"Q#{idx+1} timeout (retry {retry+1}/{MAX_RETRIES})\")\n",
    "            if retry == MAX_RETRIES - 1:\n",
    "                print(f\"Q#{idx+1}: Max retries exceeded. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Q#{idx+1} error: {e}\")\n",
    "            break\n",
    "\n",
    "# Results summary\n",
    "if rows:\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAG EVALUATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RETRIEVAL:\")\n",
    "    print(f\"  Accuracy: {correct_retrievals}/{len(qa_pairs)} = {correct_retrievals/len(qa_pairs):.2%}\")\n",
    "    \n",
    "    print(f\"\\nüìù ANSWER QUALITY:\")\n",
    "    print(f\"  Exact Match: {_avg('exact'):.2%}\")\n",
    "    print(f\"  Substring: {_avg('substring'):.2%}\")\n",
    "    print(f\"  Token Recall: {_avg('token_recall'):.3f}\")\n",
    "    print(f\"  ROUGE-1 F1: {_avg('rouge1_f'):.3f}\")\n",
    "    print(f\"  BERT Similarity: {_avg('bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    print(f\"  Avg Latency: {avg_lat:.2f}s\")\n",
    "    print(f\"  QPS: {1/avg_lat:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"benchmark_light_{len(rows)}.csv\")\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"\\n‚úì Results saved to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo valid results to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
