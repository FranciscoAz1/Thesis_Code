{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom timeouts applied: query=900 insert=300 init=120\n",
      "Server modules detected: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'Dataset' ready (text2vec-transformers + generative-openai).\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Sets up Weaviate collection using text2vec-transformers (external inference container).\n",
    "# Ensure docker-compose is up with services: weaviate + t2v-transformers.\n",
    "#   ENABLE_MODULES=text2vec-transformers,generative-openai\n",
    "#   DEFAULT_VECTORIZER_MODULE=text2vec-transformers\n",
    "#   TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080\n",
    "#   (If you previously used generative-ollama, switch to generative-openai here.)\n",
    "#\n",
    "# OpenAI setup: keep these environment variables available (e.g., in a .env file):\n",
    "#   OPENAI_API_KEY=sk-...\n",
    "#   # Optional if using a proxy or Azure-compatible endpoint:\n",
    "#   OPENAI_BASE_URL=https://api.openai.com/v1\n",
    "#   OPENAI_MODEL=gpt-4o-mini\n",
    "#\n",
    "# Timeout tuning: Some weaviate client versions expose Timeout; if not, we fall back.\n",
    "\n",
    "# Increase default timeouts to reduce GRPC RST_STREAM errors during heavy queries\n",
    "CUSTOM_INIT_TIMEOUT = 120\n",
    "CUSTOM_QUERY_TIMEOUT = 900   # 15 minutes\n",
    "CUSTOM_INSERT_TIMEOUT = 300  # 5 minutes\n",
    "\n",
    "from email import header\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "from weaviate.config import AdditionalConfig, Timeout\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "headers = {\n",
    "    \"X-Openai-Api-Key\": openai_api_key\n",
    "}\n",
    "# Attempt optional Timeout (newer weaviate client). If missing, continue with defaults.\n",
    "try:\n",
    "    TIMEOUTS = Timeout(init=CUSTOM_INIT_TIMEOUT, query=CUSTOM_QUERY_TIMEOUT, insert=CUSTOM_INSERT_TIMEOUT)\n",
    "    client = weaviate.connect_to_local(\n",
    "        additional_config=AdditionalConfig(timeout=TIMEOUTS),\n",
    "        headers=headers\n",
    "        )\n",
    "    print(\"Custom timeouts applied:\", TIMEOUTS)\n",
    "except ImportError:\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"Timeout not available in this weaviate version; using default client timeouts.\")\n",
    "    print(\"Tip: pip install --upgrade weaviate-client to enable configurable timeouts.\")\n",
    "except TypeError:\n",
    "    # Signature mismatch (older version). Reconnect without custom timeouts.\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"Timeout signature unsupported; using default timeouts.\")\n",
    "    print(\"Tip: pip install --upgrade weaviate-client to enable configurable timeouts.\")\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # for reference in prompts\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Diagnostics\n",
    "try:\n",
    "    meta = client.get_meta()\n",
    "    print(\"Server modules detected:\", list(meta.get(\"modules\", {}).keys()))\n",
    "except Exception as e:\n",
    "    print(\"Meta fetch failed:\", e)\n",
    "\n",
    "# Recreate collection for a clean slate\n",
    "try:\n",
    "    client.collections.delete(\"Dataset\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create collection with OpenAI as the generative provider (no Flex args)\n",
    "openai_model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "gen_cfg = Configure.Generative.openai(\n",
    "    model=openai_model\n",
    ")\n",
    "\n",
    "client.collections.create(\n",
    "    \"Dataset\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.LOWERCASE),\n",
    "        Property(name=\"file_path\", data_type=DataType.TEXT)\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_transformers(\n",
    "            name=\"text_vector\",\n",
    "            source_properties=[\"text\"],\n",
    "            pooling_strategy=\"masked_mean\",\n",
    "        )\n",
    "    ],\n",
    "    generative_config=gen_cfg\n",
    ")\n",
    "assert client.collections.exists(\"Dataset\")\n",
    "print(\"Collection 'Dataset' ready (text2vec-transformers + generative-openai).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 1000 docs in 5.81s\n",
      "Start RSS: 686.73 MB\n",
      "Indexing (batched)...\n",
      "Loaded 1000 docs in 5.81s\n",
      "Start RSS: 686.73 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cc81507f134992801d9c4903cfe6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 / 1000 docs in 10.73s (93.18 docs/s)\n",
      "End RSS: 585.62 MB (Œî -101.11 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\documentos_gerados\"\n",
    "WORKING_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "        if suffix == \".docx\":\n",
    "            # Read .docx files using python-docx\n",
    "            doc = Document(path)\n",
    "            text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    text.append(paragraph.text.strip())\n",
    "            return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\", \".docx\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Œî {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(\"Dataset\")\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Warming up Ollama model: qwen2.5:latest\n",
      "‚úÖ Ollama ready.\n",
      "‚úÖ Ollama ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Warm up Ollama (Optional)\n",
    "# ----------------------------------\n",
    "# Ensures the Ollama model is loaded before Weaviate calls it.\n",
    "\n",
    "# You can rerun this cell anytime after starting Ollama or changing models.\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configure warm-up behavior\n",
    "AUTO_WARMUP_OLLAMA = False\n",
    "WARMUP_TIMEOUT = 600          # seconds to wait for model to be ready\n",
    "WARMUP_POLL_INTERVAL = 5      # seconds between checks\n",
    "WARMUP_API_URL = \"http://localhost:11434/api/generate\"  # Ollama generate endpoint\n",
    "\n",
    "def warm_up_ollama(model: str | None = None, timeout_s: int = WARMUP_TIMEOUT, poll_interval: int = WARMUP_POLL_INTERVAL, prompt: str = \"ping\", api_url: str = WARMUP_API_URL) -> bool:\n",
    "    \"\"\"Ping Ollama generate API until the model is ready or timeout expires.\n",
    "    Returns True when ready, False if timed out.\"\"\"\n",
    "    mdl = model or globals().get(\"LLM_MODEL_NAME\", \"qwen2.5:latest\")\n",
    "    print(f\"üîß Warming up Ollama model: {mdl}\")\n",
    "    start = time.perf_counter()\n",
    "    last_err = None\n",
    "    while (time.perf_counter() - start) < timeout_s:\n",
    "        try:\n",
    "            resp = requests.post(api_url, json={\"model\": mdl, \"prompt\": prompt, \"stream\": False}, timeout=3000)\n",
    "            if resp.status_code == 200:\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "                text = data.get(\"response\") or data.get(\"message\") or \"\"\n",
    "                print(\"‚úÖ Ollama ready.\")\n",
    "                return True\n",
    "            else:\n",
    "                last_err = f\"HTTP {resp.status_code}: {resp.text[:200]}\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "        print(f\"‚Ä¶waiting for Ollama to load model (poll {poll_interval}s). Last error: {last_err}\")\n",
    "        time.sleep(poll_interval)\n",
    "    print(f\"‚è±Ô∏è Warm-up timed out after {timeout_s}s. Last error: {last_err}\")\n",
    "    return False\n",
    "\n",
    "# Auto warm-up when this cell runs\n",
    "if AUTO_WARMUP_OLLAMA:\n",
    "    warm_up_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 298 QA pairs from JSON.\n",
      "üöÄ Starting RAG evaluation com sa√≠da garantida em portugu√™s...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c189394b0d9a494eac9e2f3c1a172375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-best_metrics:   0%|          | 0/298 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: De acordo com o Documento Gerado #620, qual √© o percurso institucional previsto para a ratifica√ß√£o p...\n",
      "Generated: Nota: o ficheiro fornecido est√° identificado como Documento Gerado #733; respondo com base nesse texto.  - Percurso inst\n",
      "Expected: O percurso indicado √©: (1) parecer favor√°vel do Minist√©rio dos Neg√≥cios Estrangeiros, via Direc√ß√£o-Geral dos Assuntos In\n",
      "Expected File: documento_620_100.1003.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_733_100.1003.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.491', 'rouge1_f': '0.346', 'bert_cos': '0.810', 'context_token_recall': '0.533'}\n",
      "Latency: 28767.7ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q2: Qual √© o prazo final para pagamento das Notas de D√©bito emitidas e o que acontecer√° ap√≥s esse prazo,...\n",
      "Generated: Prazo: 30 dias a contar da data de recebimento da notifica√ß√£o.   Ap√≥s esse prazo: aplica√ß√£o de multas e encargos morat√≥r\n",
      "Expected: O pagamento deve ser efetuado at√© 30 de Novembro de 2023. Ap√≥s essa data, ser√£o aplicadas as penaliza√ß√µes previstas no C\n",
      "Expected File: documento_270_100.10.02.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_950_100.10.02.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.333', 'rouge1_f': '0.129', 'bert_cos': '0.644', 'context_token_recall': '0.431'}\n",
      "Latency: 18280.3ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q2: Qual √© o prazo final para pagamento das Notas de D√©bito emitidas e o que acontecer√° ap√≥s esse prazo,...\n",
      "Generated: Prazo: 30 dias a contar da data de recebimento da notifica√ß√£o.   Ap√≥s esse prazo: aplica√ß√£o de multas e encargos morat√≥r\n",
      "Expected: O pagamento deve ser efetuado at√© 30 de Novembro de 2023. Ap√≥s essa data, ser√£o aplicadas as penaliza√ß√µes previstas no C\n",
      "Expected File: documento_270_100.10.02.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_950_100.10.02.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.333', 'rouge1_f': '0.129', 'bert_cos': '0.644', 'context_token_recall': '0.431'}\n",
      "Latency: 18280.3ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q3: Qual √© a data de vencimento e o valor da Nota de D√©bito n¬∫ 2023/002, e qual o motivo dessa cobran√ßa?...\n",
      "Generated: A Nota de D√©bito n.¬∫ 2023/002 n√£o consta do documento fornecido.   Se se referia √† Nota n.¬∫ 002/2024: Valor 1.800,00 EUR\n",
      "Expected: A Nota de D√©bito n¬∫ 2023/002 vence em 12 de abril de 2023, tem valor de ‚Ç¨250,00 e o motivo √© Taxa de Ocupa√ß√£o de Espa√ßo \n",
      "Expected File: documento_518_100.10.02.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_324_100.10.02.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.391', 'rouge1_f': '0.294', 'bert_cos': '0.716', 'context_token_recall': '0.415'}\n",
      "Latency: 16393.0ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q3: Qual √© a data de vencimento e o valor da Nota de D√©bito n¬∫ 2023/002, e qual o motivo dessa cobran√ßa?...\n",
      "Generated: A Nota de D√©bito n.¬∫ 2023/002 n√£o consta do documento fornecido.   Se se referia √† Nota n.¬∫ 002/2024: Valor 1.800,00 EUR\n",
      "Expected: A Nota de D√©bito n¬∫ 2023/002 vence em 12 de abril de 2023, tem valor de ‚Ç¨250,00 e o motivo √© Taxa de Ocupa√ß√£o de Espa√ßo \n",
      "Expected File: documento_518_100.10.02.docx\n",
      "Retrieved Files: ['C:\\\\Users\\\\Francisco Azeredo\\\\Downloads\\\\gerador_documentos_gpt_azure (1)\\\\gerador_documentos_gpt_azure\\\\documentos_gerados\\\\documento_324_100.10.02.docx']\n",
      "Correct Doc Retrieved: False\n",
      "Key Metrics: {'exact': False, 'substring': False, 'token_recall': '0.391', 'rouge1_f': '0.294', 'bert_cos': '0.716', 'context_token_recall': '0.415'}\n",
      "Latency: 16393.0ms\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "RAG EVALUATION RESULTS - BEST METRICS ONLY\n",
      "============================================================\n",
      "\n",
      "üéØ RETRIEVAL PERFORMANCE:\n",
      "  Document Retrieval Accuracy: 40/298 = 13.42%\n",
      "  Average Retrieval Rank: -0.9\n",
      "\n",
      "üìù ANSWER QUALITY:\n",
      "  Exact Match: 0.00%\n",
      "  Substring Match: 0.00%\n",
      "  Token Recall: 0.438\n",
      "  ROUGE-1 F1: 0.270\n",
      "  BERT Similarity: 0.659\n",
      "\n",
      "üîç CONTEXT QUALITY:\n",
      "  Context Token Recall: 0.475\n",
      "  Context ROUGE-1 F1: 0.263\n",
      "\n",
      "‚ö° PERFORMANCE:\n",
      "  Average Latency: 23987.0ms\n",
      "  95th Percentile Latency: 41808.4ms\n",
      "  Questions per Second: 0.04\n",
      "\n",
      "üíæ Results saved to: C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\best_results_best_metrics303.csv\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "\n",
      "============================================================\n",
      "RAG EVALUATION RESULTS - BEST METRICS ONLY\n",
      "============================================================\n",
      "\n",
      "üéØ RETRIEVAL PERFORMANCE:\n",
      "  Document Retrieval Accuracy: 40/298 = 13.42%\n",
      "  Average Retrieval Rank: -0.9\n",
      "\n",
      "üìù ANSWER QUALITY:\n",
      "  Exact Match: 0.00%\n",
      "  Substring Match: 0.00%\n",
      "  Token Recall: 0.438\n",
      "  ROUGE-1 F1: 0.270\n",
      "  BERT Similarity: 0.659\n",
      "\n",
      "üîç CONTEXT QUALITY:\n",
      "  Context Token Recall: 0.475\n",
      "  Context ROUGE-1 F1: 0.263\n",
      "\n",
      "‚ö° PERFORMANCE:\n",
      "  Average Latency: 23987.0ms\n",
      "  95th Percentile Latency: 41808.4ms\n",
      "  Questions per Second: 0.04\n",
      "\n",
      "üíæ Results saved to: C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\best_results_best_metrics303.csv\n",
      "\n",
      "‚úÖ Evaluation complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation - Best Metrics Only\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Optimized to focus on the most meaningful metrics for RAG evaluation.\n",
    "\n",
    "from pickle import FALSE\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from regex import F\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration - Using Relative Paths --------\n",
    "# Updated to use relative paths after reorganization\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "QA_JSON_PATH = str(PROJECT_ROOT / \"datasets\" / \"qa_dataset300.json\")\n",
    "OUTPUT_CSV_PATH = str(PROJECT_ROOT / \"results_csv\")  # set to None to skip saving\n",
    "TOP_K = 3            # lower to reduce vector fetch time\n",
    "MAX_Q = None         # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True  # semantic metrics cost\n",
    "PER_QUERY_DEADLINE = 60000.0  # seconds, must be < client.query timeout\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF = 10.0  # seconds added each retry\n",
    "LANGUAGE = \"pt-PT\"\n",
    "PROMPT_PREFIX = \"Responda de forma breve e objetiva em portugu√™s (pt-PT): \"\n",
    "COT_PROMPT = False  # chain-of-thought prompting (slower, may improve complex Qs)\n",
    "AUTO_WARMUP_OLLAMA = globals().get('AUTO_WARMUP_OLLAMA', True)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"‚úì Paths configured:\")\n",
    "print(f\"  Q&A Dataset: {QA_JSON_PATH}\")\n",
    "print(f\"  Output Folder: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  Dataset exists: {Path(QA_JSON_PATH).exists()}\")\n",
    "print(f\"  Output folder exists: {Path(OUTPUT_CSV_PATH).exists()}\")\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def tokenize_pt(s: str):\n",
    "    return [t.lower() for t in TOKEN_SPLIT_RE.split(s) if t.strip()]\n",
    "\n",
    "def token_recall(answer: str, gold: str) -> float:\n",
    "    at = set(tokenize_pt(answer))\n",
    "    gt = set(tokenize_pt(gold))\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    return len(at & gt) / len(gt)\n",
    "\n",
    "def compute_rouge1(hyp: str, ref: str):\n",
    "    if not hyp.strip() or not ref.strip():\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "    try:\n",
    "        return _lazy_rouge().get_scores(hyp, ref)[0]\n",
    "    except:\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "\n",
    "def compute_bleu(hyp: str, ref: str) -> float:\n",
    "    h_toks = tokenize_pt(hyp)\n",
    "    r_toks = tokenize_pt(ref)\n",
    "    if not h_toks or not r_toks:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return sentence_bleu([r_toks], h_toks, smoothing_function=_SMOOTH)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_bert_similarity(hyp: str, ref: str) -> float:\n",
    "    if not USE_BERT_SIM or not hyp.strip() or not ref.strip():\n",
    "        return 0.0\n",
    "    model = _lazy_bert()\n",
    "    emb = model.encode([hyp, ref])\n",
    "    return float(np.dot(emb[0], emb[1]) / (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1])))\n",
    "\n",
    "def best_sentence_overlap(answer: str, gold: str) -> float:\n",
    "    sents_a = sent_tokenize(answer, language=\"portuguese\")\n",
    "    sents_g = sent_tokenize(gold, language=\"portuguese\")\n",
    "    best = 0.0\n",
    "    for sa in sents_a:\n",
    "        for sg in sents_g:\n",
    "            tr = token_recall(sa, sg)\n",
    "            if tr > best:\n",
    "                best = tr\n",
    "    return best\n",
    "\n",
    "# Load QA\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_JSON_PATH):\n",
    "    with open(QA_JSON_PATH, encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "        for item in qa_data:\n",
    "            if \"pergunta\" in item and \"resposta\" in item:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": item[\"pergunta\"].strip(),\n",
    "                    \"gold_answer\": item[\"resposta\"].strip(),\n",
    "                    \"context\": item.get(\"contexto\",\"\").strip(),\n",
    "                    \"file\": item.get(\"arquivo\",\"\").strip()\n",
    "                })\n",
    "else:\n",
    "    print(\"QA JSON not found. Check QA_JSON_PATH.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    random.shuffle(qa_pairs)\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} Q&A pairs.\")\n",
    "\n",
    "# Optional warmup\n",
    "if AUTO_WARMUP_OLLAMA:\n",
    "    warm_up_ollama()\n",
    "\n",
    "rows = []\n",
    "latencies = []\n",
    "correct_retrievals = 0\n",
    "cot_phrase = \" Vamos pensar passo a passo.\" if COT_PROMPT else \"\"\n",
    "\n",
    "for idx, pair in enumerate(tqdm(qa_pairs, desc=\"Evaluating (Best Metrics)\")):\n",
    "    q = pair[\"question\"]\n",
    "    gold_answer = pair[\"gold_answer\"]\n",
    "    gold_context = pair[\"context\"]\n",
    "    gold_file = pair[\"file\"]\n",
    "    \n",
    "    start_t = time.perf_counter()\n",
    "    try:\n",
    "        response_obj = rag.query(q + cot_phrase, param={\"limit\": TOP_K})\n",
    "        \n",
    "        if not response_obj or not hasattr(response_obj, \"answer\"):\n",
    "            print(f\"Q#{idx+1}: No valid response. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        answer_text = response_obj.answer.strip() if response_obj.answer else \"\"\n",
    "        retrieved_context = \" \".join([ctx.get(\"content\", \"\") for ctx in response_obj.context]) if hasattr(response_obj, \"context\") else \"\"\n",
    "        \n",
    "        # Retrieval metrics\n",
    "        retrieved_files = [ctx.get(\"file_path\", \"\") for ctx in response_obj.context] if hasattr(response_obj, \"context\") else []\n",
    "        rank = retrieved_files.index(gold_file) if gold_file in retrieved_files else -1\n",
    "        if rank >= 0:\n",
    "            correct_retrievals += 1\n",
    "        \n",
    "        # Answer quality (best metrics)\n",
    "        exact = 1.0 if answer_text.lower() == gold_answer.lower() else 0.0\n",
    "        substring = 1.0 if gold_answer.lower() in answer_text.lower() else 0.0\n",
    "        tok_recall = token_recall(answer_text, gold_answer)\n",
    "        rouge1_scores = compute_rouge1(answer_text, gold_answer)\n",
    "        rouge1_f = rouge1_scores[\"rouge-1\"][\"f\"]\n",
    "        bert_cos = compute_bert_similarity(answer_text, gold_answer) if USE_BERT_SIM else 0.0\n",
    "        \n",
    "        # Context quality\n",
    "        context_token_recall = token_recall(retrieved_context, gold_context) if gold_context else 0.0\n",
    "        context_rouge1 = compute_rouge1(retrieved_context, gold_context) if gold_context else {\"rouge-1\": {\"f\": 0.0}}\n",
    "        context_bert = compute_bert_similarity(retrieved_context, gold_context) if USE_BERT_SIM and gold_context else 0.0\n",
    "        \n",
    "        lat = time.perf_counter() - start_t\n",
    "        latencies.append(lat)\n",
    "        \n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"generated_answer\": answer_text,\n",
    "            \"gold_file\": gold_file,\n",
    "            \"retrieved_files\": \"|\".join(retrieved_files),\n",
    "            \"retrieval_rank\": rank,\n",
    "            \"exact\": exact,\n",
    "            \"substring\": substring,\n",
    "            \"token_recall\": tok_recall,\n",
    "            \"rouge1_f\": rouge1_f,\n",
    "            \"bert_cos\": bert_cos,\n",
    "            \"context_token_recall\": context_token_recall,\n",
    "            \"context_rouge1_f\": context_rouge1[\"rouge-1\"][\"f\"],\n",
    "            \"context_bert_cos\": context_bert,\n",
    "            \"latency_s\": lat\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Q#{idx+1} error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Results summary\n",
    "if rows:\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    \n",
    "    # Report best metrics only\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAG EVALUATION RESULTS - BEST METRICS ONLY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RETRIEVAL PERFORMANCE:\")\n",
    "    print(f\"  Document Retrieval Accuracy: {correct_retrievals}/{len(qa_pairs)} = {correct_retrievals/len(qa_pairs):.2%}\")\n",
    "    print(f\"  Average Retrieval Rank: {_avg('retrieval_rank'):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìù ANSWER QUALITY:\")\n",
    "    print(f\"  Exact Match: {_avg('exact'):.2%}\")\n",
    "    print(f\"  Substring Match: {_avg('substring'):.2%}\")\n",
    "    print(f\"  Token Recall: {_avg('token_recall'):.3f}\")\n",
    "    print(f\"  ROUGE-1 F1: {_avg('rouge1_f'):.3f}\")\n",
    "    if 'bert_cos' in rows[0]:\n",
    "        print(f\"  BERT Similarity: {_avg('bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüîç CONTEXT QUALITY:\")\n",
    "    if 'context_token_recall' in rows[0]:\n",
    "        print(f\"  Context Token Recall: {_avg('context_token_recall'):.3f}\")\n",
    "    if 'context_rouge1_f' in rows[0]:\n",
    "        print(f\"  Context ROUGE-1 F1: {_avg('context_rouge1_f'):.3f}\")\n",
    "    if 'context_bert_cos' in rows[0]:\n",
    "        print(f\"  Context BERT Similarity: {_avg('context_bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95 = statistics.quantiles(latencies, n=20)[18] if len(latencies)>=20 else max(latencies)\n",
    "    print(f\"  Average Latency: {avg_lat*1000:.1f}ms\")\n",
    "    print(f\"  95th Percentile Latency: {p95*1000:.1f}ms\")\n",
    "    print(f\"  Questions per Second: {1/avg_lat:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        mode = \"CoT\" if COT_PROMPT else \"Naive\"\n",
    "        n = len(rows)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"weaviateNaive{TOP_K}_{n}.csv\")\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"\\n‚úì Results saved to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo valid results to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76652161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing connections...\n",
      "‚úÖ Weaviate client connected\n",
      "‚úÖ Weaviate client connected\n",
      "‚úÖ Ollama is running with 5 models\n",
      "‚úÖ qwen2.5 model is available\n",
      "‚úÖ Weaviate is running, modules: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n",
      "‚úÖ Dataset collection has 1000 documents\n",
      "‚úÖ Vector search is working\n",
      "‚úÖ Ollama is running with 5 models\n",
      "‚úÖ qwen2.5 model is available\n",
      "‚úÖ Weaviate is running, modules: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n",
      "‚úÖ Dataset collection has 1000 documents\n",
      "‚úÖ Vector search is working\n",
      "‚úÖ Generative search is working and responding in Portuguese\n",
      "\n",
      "üéØ If all tests pass, you can proceed with the benchmark.\n",
      "üéØ If tests fail, fix the issues before running the evaluation.\n",
      "‚úÖ Generative search is working and responding in Portuguese\n",
      "\n",
      "üéØ If all tests pass, you can proceed with the benchmark.\n",
      "üéØ If tests fail, fix the issues before running the evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Test connections before running benchmark\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üîç Testing connections...\")\n",
    "import weaviate\n",
    "from weaviate.config import AdditionalConfig, Timeout\n",
    "# Connect to Weaviate with custom timeouts\n",
    "\n",
    "CUSTOM_INIT_TIMEOUT = 120\n",
    "CUSTOM_QUERY_TIMEOUT = 900   # 15 minutes\n",
    "CUSTOM_INSERT_TIMEOUT = 300  # 5 minutes\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "headers = {\n",
    "    \"X-Openai-Api-Key\": openai_api_key\n",
    "}\n",
    "try:\n",
    "    TIMEOUTS = Timeout(init=CUSTOM_INIT_TIMEOUT, query=CUSTOM_QUERY_TIMEOUT, insert=CUSTOM_INSERT_TIMEOUT)\n",
    "    client = weaviate.connect_to_local(additional_config=AdditionalConfig(timeout=TIMEOUTS), headers=headers)\n",
    "    print(\"‚úÖ Weaviate client connected\")\n",
    "except Exception as e:\n",
    "    print(\"Weaviate timeout config error:\", e)\n",
    "# Test Ollama\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(f\"‚úÖ Ollama is running with {len(models)} models\")\n",
    "        qwen_available = any('qwen2.5' in model.get('name', '') for model in models)\n",
    "        if qwen_available:\n",
    "            print(\"‚úÖ qwen2.5 model is available\")\n",
    "        else:\n",
    "            print(\"‚ùå qwen2.5 model not found - run: ollama pull qwen2.5:latest\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama responded with status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection failed: {e}\")\n",
    "    print(\"Run: ollama serve\")\n",
    "\n",
    "# Test Weaviate\n",
    "try:\n",
    "    if 'client' in globals():\n",
    "        meta = client.get_meta()\n",
    "        print(f\"‚úÖ Weaviate is running, modules: {list(meta.get('modules', {}).keys())}\")\n",
    "        \n",
    "        # Test collection exists\n",
    "        if client.collections.exists(\"Dataset\"):\n",
    "            collection = client.collections.get(\"Dataset\")\n",
    "            aggregate = collection.aggregate.over_all(total_count=True)\n",
    "            count = aggregate.total_count\n",
    "            print(f\"‚úÖ Dataset collection has {count} documents\")\n",
    "            \n",
    "            # Simple test query (no generation)\n",
    "            try:\n",
    "                test_result = collection.query.near_text(\n",
    "                    query=\"teste\", \n",
    "                    limit=1,\n",
    "                    return_metadata=['distance']\n",
    "                )\n",
    "                print(\"‚úÖ Vector search is working\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Vector search failed: {e}\")\n",
    "            # Simple test generation (Portuguese enforced)\n",
    "            try:\n",
    "                gen_result = collection.generate.hybrid(\n",
    "                    query=\"O que √© IA?\", \n",
    "                    limit=1,\n",
    "                    target_vector=\"text_vector\",\n",
    "                    grouped_task=\"Explique brevemente em portugu√™s (pt-PT): O que √© IA?\",\n",
    "                    return_metadata=['distance']\n",
    "                )\n",
    "                gen_text = getattr(getattr(gen_result, 'generative', None), 'text', None)\n",
    "                if gen_text:\n",
    "                    if any('\\u4e00' <= ch <= '\\u9fff' for ch in gen_text):\n",
    "                        print(\"‚ö†Ô∏è Generative search returned Chinese text; prompts will enforce pt-BR in the evaluation cell.\")\n",
    "                    else:\n",
    "                        print(\"‚úÖ Generative search is working and responding in Portuguese\")\n",
    "                else:\n",
    "                    print(\"‚ùå Generative search returned no text\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generative search failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå Dataset collection not found\")\n",
    "    else:\n",
    "        print(\"‚ùå Weaviate client not initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Weaviate connection failed: {e}\")\n",
    "\n",
    "print(\"\\nüéØ If all tests pass, you can proceed with the benchmark.\")\n",
    "print(\"üéØ If tests fail, fix the issues before running the evaluation.\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
