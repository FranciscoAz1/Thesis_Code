{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\Francisco Azeredo\\\\.conda\\\\envs\\\\tese',\n",
       " 'c:\\\\Users\\\\Francisco Azeredo\\\\.conda\\\\envs\\\\tese\\\\Lib\\\\site-packages']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import site\n",
    "site.getsitepackages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DemoCollection': _CollectionConfig(name='DemoCollection', description=None, generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='page', description=None, data_type=<DataType.INT: 'int'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=None, vectorizer='none'), _Property(name='text', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['page', 'text']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Fluxo': _CollectionConfig(name='Fluxo', description='A workflow that contains multiple stages (etapas).', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='hasEtapas', description=None, target_collections=['Etapa']), _ReferenceProperty(name='belongsToFicheiros', description=None, target_collections=['Ficheiro']), _ReferenceProperty(name='belongsToPastas', description=None, target_collections=['Pasta'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Etapa': _CollectionConfig(name='Etapa', description='A stage within a workflow.', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='belongsToFluxo', description=None, target_collections=['Fluxo']), _ReferenceProperty(name='hasFicheiros', description=None, target_collections=['Ficheiro'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Entidade': _CollectionConfig(name='Entidade', description='An entity that owns folders (pastas).', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='hasFicheiros', description=None, target_collections=['Ficheiro']), _ReferenceProperty(name='hasPastas', description=None, target_collections=['Pasta'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Pasta': _CollectionConfig(name='Pasta', description='A folder belonging to an entity and containing documents (ficheiros).', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='hasFicheiros', description=None, target_collections=['Ficheiro']), _ReferenceProperty(name='hasEntidades', description=None, target_collections=['Entidade']), _ReferenceProperty(name='hasFluxos', description=None, target_collections=['Fluxo'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Ficheiro': _CollectionConfig(name='Ficheiro', description='A document that contains metadata.', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='belongsToMetadados', description=None, target_collections=['Metadados']), _ReferenceProperty(name='hasEtapas', description=None, target_collections=['Etapa']), _ReferenceProperty(name='hasPastas', description=None, target_collections=['Pasta']), _ReferenceProperty(name='hasEntidades', description=None, target_collections=['Entidade'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))}), 'Metadados': _CollectionConfig(name='Metadados', description='Metadata associated with a document (ficheiro).', generative_config=_GenerativeConfig(generative=<GenerativeSearches.OLLAMA: 'generative-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'llama3.2'}), inverted_index_config=_InvertedIndexConfig(bm25=_BM25Config(b=0.75, k1=1.2), cleanup_interval_seconds=60, index_null_state=False, index_property_length=False, index_timestamps=False, stopwords=_StopwordsConfig(preset=<StopwordsPreset.EN: 'en'>, additions=None, removals=None)), multi_tenancy_config=_MultiTenancyConfig(enabled=False, auto_tenant_creation=False, auto_tenant_activation=False), properties=[_Property(name='name', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=None, vectorizer='none')], references=[_ReferenceProperty(name='hasFicheiros', description=None, target_collections=['Ficheiro']), _ReferenceProperty(name='hasEtapas', description=None, target_collections=['Etapa']), _ReferenceProperty(name='hasPastas', description=None, target_collections=['Pasta']), _ReferenceProperty(name='hasEntidades', description=None, target_collections=['Entidade'])], replication_config=_ReplicationConfig(factor=1, async_enabled=False, deletion_strategy=<ReplicationDeletionStrategy.NO_AUTOMATED_RESOLUTION: 'NoAutomatedResolution'>), reranker_config=None, sharding_config=_ShardingConfig(virtual_per_physical=128, desired_count=1, actual_count=1, desired_virtual_count=128, actual_virtual_count=128, key='_id', strategy='hash', function='murmur3'), vector_index_config=None, vector_index_type=None, vectorizer_config=None, vectorizer=None, vector_config={'title_vector': _NamedVectorConfig(vectorizer=_NamedVectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'mxbai-embed-large', 'vectorizeClassName': True}, source_properties=['name']), vector_index_config=_VectorIndexConfigHNSW(multi_vector=None, quantizer=None, cleanup_interval_seconds=300, distance_metric=<VectorDistances.COSINE: 'cosine'>, dynamic_ef_min=100, dynamic_ef_max=500, dynamic_ef_factor=8, ef=-1, ef_construction=128, filter_strategy=<VectorFilterStrategy.SWEEPING: 'sweeping'>, flat_search_cutoff=40000, max_connections=32, skip=False, vector_cache_max_objects=1000000000000))})}\n"
     ]
    }
   ],
   "source": [
    "response = client.collections.list_all(simple=False)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections with vectorization & generation created successfully!\n",
      "References added successfully!\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.config import Property, DataType, ReferenceProperty, Configure\n",
    "\n",
    "# Define Ollama API details\n",
    "OLLAMA_API_ENDPOINT = \"http://host.docker.internal:11434\"\n",
    "VECTOR_MODEL = \"mxbai-embed-large\"\n",
    "GENERATION_MODEL = \"llama3.2\"\n",
    "\n",
    "# Delete old collections if they exist\n",
    "for collection_name in [\"Fluxo\", \"Etapa\", \"Entidade\", \"Pasta\", \"Ficheiro\", \"Metadados\"]:\n",
    "    try:\n",
    "        client.collections.delete(collection_name)\n",
    "    except:\n",
    "        pass  # Ignore if collection doesn't exist\n",
    "\n",
    "# Function to create a collection with vectorization & generative AI\n",
    "def create_collection(name, description, properties):\n",
    "    client.collections.create(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        properties=properties,\n",
    "        vectorizer_config=[\n",
    "            Configure.NamedVectors.text2vec_ollama(\n",
    "                name=\"title_vector\",\n",
    "                source_properties=[\"name\"],  # Change to relevant properties\n",
    "                api_endpoint=OLLAMA_API_ENDPOINT,\n",
    "                model=VECTOR_MODEL\n",
    "            )\n",
    "        ],\n",
    "        generative_config=Configure.Generative.ollama(\n",
    "            api_endpoint=OLLAMA_API_ENDPOINT,\n",
    "            model=GENERATION_MODEL\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 1. Create \"Fluxo\" Collection\n",
    "create_collection(\n",
    "    \"Fluxo\",\n",
    "    \"A workflow that contains multiple stages (etapas).\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "# 2. Create \"Etapa\" Collection\n",
    "create_collection(\n",
    "    \"Etapa\",\n",
    "    \"A stage within a workflow.\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "# 3. Create \"Entidade\" Collection\n",
    "create_collection(\n",
    "    \"Entidade\",\n",
    "    \"An entity that owns folders (pastas).\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "# 4. Create \"Pasta\" Collection\n",
    "create_collection(\n",
    "    \"Pasta\",\n",
    "    \"A folder belonging to an entity and containing documents (ficheiros).\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "# 5. Create \"Ficheiro\" Collection\n",
    "create_collection(\n",
    "    \"Ficheiro\",\n",
    "    \"A document that contains metadata.\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "# 6. Create \"Metadados\" Collection\n",
    "create_collection(\n",
    "    \"Metadados\",\n",
    "    \"Metadata associated with a document (ficheiro).\",\n",
    "    [Property(name=\"name\", data_type=DataType.TEXT)]\n",
    ")\n",
    "\n",
    "print(\"Collections with vectorization & generation created successfully!\")\n",
    "\n",
    "# Create reference properties\n",
    "# Get collection objects\n",
    "fluxo = client.collections.get(\"Fluxo\")\n",
    "etapa = client.collections.get(\"Etapa\")\n",
    "entidade = client.collections.get(\"Entidade\")\n",
    "pasta = client.collections.get(\"Pasta\")\n",
    "ficheiro = client.collections.get(\"Ficheiro\")\n",
    "metadados = client.collections.get(\"Metadados\")\n",
    "\n",
    "# Add cross-references\n",
    "fluxo.config.add_reference(ReferenceProperty(name=\"hasEtapas\", target_collection=\"Etapa\"))\n",
    "fluxo.config.add_reference(ReferenceProperty(name=\"belongsToFicheiros\", target_collection=\"Ficheiro\"))\n",
    "fluxo.config.add_reference(ReferenceProperty(name=\"belongsToPastas\", target_collection=\"Pasta\"))\n",
    "\n",
    "etapa.config.add_reference( ReferenceProperty(name=\"belongsToFluxo\", target_collection=\"Fluxo\"))\n",
    "etapa.config.add_reference( ReferenceProperty(name=\"hasFicheiros\", target_collection=\"Ficheiro\"))\n",
    "\n",
    "entidade.config.add_reference( ReferenceProperty(name=\"hasFicheiros\", target_collection=\"Ficheiro\"))\n",
    "entidade.config.add_reference( ReferenceProperty(name=\"hasPastas\", target_collection=\"Pasta\"))\n",
    "\n",
    "pasta.config.add_reference( ReferenceProperty(name=\"hasFicheiros\", target_collection=\"Ficheiro\"))\n",
    "pasta.config.add_reference( ReferenceProperty(name=\"hasEntidades\", target_collection=\"Entidade\"))\n",
    "pasta.config.add_reference( ReferenceProperty(name=\"hasFluxos\", target_collection=\"Fluxo\"))\n",
    "\n",
    "ficheiro.config.add_reference( ReferenceProperty(name=\"belongsToMetadados\", target_collection=\"Metadados\"))\n",
    "ficheiro.config.add_reference( ReferenceProperty(name=\"hasEtapas\", target_collection=\"Etapa\"))\n",
    "ficheiro.config.add_reference( ReferenceProperty(name=\"hasPastas\", target_collection=\"Pasta\"))\n",
    "ficheiro.config.add_reference( ReferenceProperty(name=\"hasEntidades\", target_collection=\"Entidade\"))\n",
    "\n",
    "\n",
    "metadados.config.add_reference( ReferenceProperty(name=\"hasFicheiros\", target_collection=\"Ficheiro\"))\n",
    "metadados.config.add_reference( ReferenceProperty(name=\"hasEtapas\", target_collection=\"Etapa\"))\n",
    "metadados.config.add_reference( ReferenceProperty(name=\"hasPastas\", target_collection=\"Pasta\"))\n",
    "metadados.config.add_reference( ReferenceProperty(name=\"hasEntidades\", target_collection=\"Entidade\"))\n",
    "print(\"References added successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk sample data inserted and linked successfully!\n"
     ]
    }
   ],
   "source": [
    "def add_fluxo(fluxo_name,fluxo ,pasta_obj=None, ficheiro_obj=None, etapa_obj=None):\n",
    "    \"\"\"Creates a Fluxo and links it to Pasta and Ficheiro if provided.\"\"\"\n",
    "    fluxo_obj = fluxo.data.insert({\"name\": fluxo_name})\n",
    "    if pasta_obj:\n",
    "        fluxo.data.reference_add(fluxo_obj, \"belongsToPastas\", pasta_obj)\n",
    "    if ficheiro_obj:\n",
    "        fluxo.data.reference_add(fluxo_obj, \"belongsToFicheiros\", ficheiro_obj)\n",
    "    if etapa_obj:\n",
    "        fluxo.data.reference_add(fluxo_obj, \"hasEtapas\", etapa_obj)\n",
    "    return fluxo_obj\n",
    "\n",
    "def add_etapa(etapa_name, etapa, fluxo_obj=None, ficheiro_obj=None):\n",
    "    etapa_obj = etapa.data.insert({\"name\": etapa_name})\n",
    "    if fluxo_obj:\n",
    "        fluxo.data.reference_add(fluxo_obj, \"hasEtapas\", etapa_obj)\n",
    "        etapa.data.reference_add(etapa_obj, \"belongsToFluxo\", fluxo_obj)\n",
    "    if ficheiro_obj:\n",
    "        etapa.data.reference_add(etapa_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    return etapa_obj\n",
    "\n",
    "def add_entidade(entidade_name,entidade, ficheiro_obj=None, pasta_obj=None):\n",
    "    entidade_obj = entidade.data.insert({\"name\": entidade_name})\n",
    "    if ficheiro_obj:\n",
    "        entidade.data.reference_add(entidade_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    if pasta_obj:\n",
    "        entidade.data.reference_add(entidade_obj, \"hasPastas\", pasta_obj)\n",
    "    return entidade_obj\n",
    "\n",
    "def add_pasta(pasta_name, pasta, entidade_obj=None, fluxo_obj=None, ficheiro_obj=None):\n",
    "    pasta_obj = pasta.data.insert({\"name\": pasta_name})\n",
    "    if entidade_obj:\n",
    "        entidade.data.reference_add(entidade_obj, \"hasPastas\", pasta_obj)\n",
    "        pasta.data.reference_add(pasta_obj, \"hasEntidades\", entidade_obj)\n",
    "    if fluxo_obj:\n",
    "        pasta.data.reference_add(pasta_obj, \"hasFluxos\", fluxo_obj)\n",
    "    if ficheiro_obj:\n",
    "        pasta.data.reference_add(pasta_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    return pasta_obj\n",
    "\n",
    "def add_ficheiro(ficheiro_name, ficheiro, pasta_obj=None, entidade_obj=None, etapa_obj=None, metadados_obj=None):\n",
    "    ficheiro_obj = ficheiro.data.insert({\"nome\": ficheiro_name})\n",
    "    if pasta_obj:\n",
    "        pasta.data.reference_add(pasta_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    if entidade_obj:\n",
    "        entidade.data.reference_add(entidade_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    if etapa_obj:\n",
    "        etapa.data.reference_add(etapa_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    if metadados_obj:\n",
    "        ficheiro.data.reference_add(ficheiro_obj, \"belongsToMetadados\", metadados_obj)\n",
    "    return ficheiro_obj\n",
    "\n",
    "def add_metadados(metadados_data, metadados, ficheiro_obj=None, etapa_obj=None, pasta_obj=None, entidade_obj=None):\n",
    "    metadados_obj = metadados.data.insert({\"dados\": metadados_data})\n",
    "    if ficheiro_obj:\n",
    "        metadados.data.reference_add(metadados_obj, \"hasFicheiros\", ficheiro_obj)\n",
    "    if etapa_obj:\n",
    "        metadados.data.reference_add(metadados_obj, \"hasEtapas\", etapa_obj)\n",
    "    if pasta_obj:\n",
    "        metadados.data.reference_add(metadados_obj, \"hasPastas\", pasta_obj)\n",
    "    if entidade_obj:\n",
    "        metadados.data.reference_add(metadados_obj, \"hasEntidades\", entidade_obj)\n",
    "    return metadados_obj\n",
    "\n",
    "# Configuration for data size\n",
    "num_entidades = 2\n",
    "num_pastas_per_entidade = 5\n",
    "num_ficheiros_per_pasta = 8\n",
    "num_metadados_per_ficheiro = 1\n",
    "num_fluxos = 5\n",
    "num_etapas_per_fluxo = 10\n",
    "\n",
    "# Step 1: Create Entidades\n",
    "entidade = client.collections.get(\"Entidade\")\n",
    "entidade_objs = [add_entidade(f\"Empresa {i+1}\",entidade) for i in range(num_entidades)]\n",
    "\n",
    "# Step 2: Create Pastas and link them to Entidades\n",
    "pasta_objs = []\n",
    "pasta = client.collections.get(\"Pasta\")\n",
    "for i, entidade_obj in enumerate(entidade_objs):\n",
    "    for j in range(num_pastas_per_entidade):\n",
    "        pasta_obj = add_pasta(f\"Pasta {j+1} of Empresa {i+1}\",pasta , entidade_obj=entidade_obj)\n",
    "        pasta_objs.append(pasta_obj)\n",
    "\n",
    "# Step 3: Create Ficheiros and link them to Pastas and Entidades\n",
    "ficheiro_objs = []\n",
    "ficheiro = client.collections.get(\"Ficheiro\")\n",
    "for i, pasta_obj in enumerate(pasta_objs):\n",
    "    entidade_obj = entidade_objs[i % len(entidade_objs)]\n",
    "    for j in range(num_ficheiros_per_pasta):\n",
    "        ficheiro_obj = add_ficheiro(f\"Documento {j+1} in {pasta_obj}\",ficheiro, pasta_obj=pasta_obj, entidade_obj=entidade_obj)\n",
    "        ficheiro_objs.append(ficheiro_obj)\n",
    "\n",
    "# Step 4: Create Metadados and link them to Ficheiros\n",
    "metadados_objs = []\n",
    "metadados = client.collections.get(\"Metadados\")\n",
    "for i, ficheiro_obj in enumerate(ficheiro_objs):\n",
    "    for j in range(num_metadados_per_ficheiro):\n",
    "        metadados_obj = add_metadados(f\"Metadata {j+1} for {ficheiro_obj}\",metadados,ficheiro_obj=ficheiro_obj)\n",
    "        metadados_objs.append(metadados_obj)\n",
    "\n",
    "# Step 5: Create Fluxos and link them to Pastas and Ficheiros\n",
    "fluxo_objs = []\n",
    "fluxo = client.collections.get(\"Fluxo\")\n",
    "for i in range(num_fluxos):\n",
    "    pasta_obj = pasta_objs[i % len(pasta_objs)]\n",
    "    ficheiro_obj = ficheiro_objs[i % len(ficheiro_objs)]\n",
    "    fluxo_obj = add_fluxo(f\"Fluxo {i+1}\",fluxo, pasta_obj=pasta_obj, ficheiro_obj=ficheiro_obj)\n",
    "    fluxo_objs.append(fluxo_obj)\n",
    "\n",
    "# Step 6: Create Etapas and link them to Fluxos and Ficheiros\n",
    "etapa = client.collections.get(\"Etapa\")\n",
    "etapa_objs = []\n",
    "for i, fluxo_obj in enumerate(fluxo_objs):\n",
    "    ficheiro_obj = ficheiro_objs[i % len(ficheiro_objs)]\n",
    "    for j in range(num_etapas_per_fluxo):\n",
    "        etapa_obj = add_etapa(f\"Etapa {j+1} of Fluxo {i+1}\", etapa,fluxo_obj=fluxo_obj, ficheiro_obj=ficheiro_obj)\n",
    "        etapa_objs.append(etapa_obj)\n",
    "\n",
    "print(\"Bulk sample data inserted and linked successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying Cross Referenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluxo: Fluxo 1\n",
      "obj.references: dict_keys(['hasEtapas'])\n",
      "  - Etapa: Etapa 1 of Fluxo 1\n",
      "  - Etapa: Etapa 2 of Fluxo 1\n",
      "  - Etapa: Etapa 3 of Fluxo 1\n",
      "  - Etapa: Etapa 4 of Fluxo 1\n",
      "  - Etapa: Etapa 5 of Fluxo 1\n",
      "  - Etapa: Etapa 6 of Fluxo 1\n",
      "  - Etapa: Etapa 7 of Fluxo 1\n",
      "  - Etapa: Etapa 8 of Fluxo 1\n",
      "  - Etapa: Etapa 9 of Fluxo 1\n",
      "  - Etapa: Etapa 10 of Fluxo 1\n",
      "Fluxo: Fluxo 2\n",
      "obj.references: dict_keys(['hasEtapas'])\n",
      "  - Etapa: Etapa 1 of Fluxo 2\n",
      "  - Etapa: Etapa 2 of Fluxo 2\n",
      "  - Etapa: Etapa 3 of Fluxo 2\n",
      "  - Etapa: Etapa 4 of Fluxo 2\n",
      "  - Etapa: Etapa 5 of Fluxo 2\n",
      "  - Etapa: Etapa 6 of Fluxo 2\n",
      "  - Etapa: Etapa 7 of Fluxo 2\n",
      "  - Etapa: Etapa 8 of Fluxo 2\n",
      "  - Etapa: Etapa 9 of Fluxo 2\n",
      "  - Etapa: Etapa 10 of Fluxo 2\n",
      "Fluxo: Fluxo 3\n",
      "obj.references: dict_keys(['hasEtapas'])\n",
      "  - Etapa: Etapa 1 of Fluxo 3\n",
      "  - Etapa: Etapa 2 of Fluxo 3\n",
      "  - Etapa: Etapa 3 of Fluxo 3\n",
      "  - Etapa: Etapa 4 of Fluxo 3\n",
      "  - Etapa: Etapa 5 of Fluxo 3\n",
      "  - Etapa: Etapa 6 of Fluxo 3\n",
      "  - Etapa: Etapa 7 of Fluxo 3\n",
      "  - Etapa: Etapa 8 of Fluxo 3\n",
      "  - Etapa: Etapa 9 of Fluxo 3\n",
      "  - Etapa: Etapa 10 of Fluxo 3\n",
      "Fluxo: Fluxo 4\n",
      "obj.references: dict_keys(['hasEtapas'])\n",
      "  - Etapa: Etapa 1 of Fluxo 4\n",
      "  - Etapa: Etapa 2 of Fluxo 4\n",
      "  - Etapa: Etapa 3 of Fluxo 4\n",
      "  - Etapa: Etapa 4 of Fluxo 4\n",
      "  - Etapa: Etapa 5 of Fluxo 4\n",
      "  - Etapa: Etapa 6 of Fluxo 4\n",
      "  - Etapa: Etapa 7 of Fluxo 4\n",
      "  - Etapa: Etapa 8 of Fluxo 4\n",
      "  - Etapa: Etapa 9 of Fluxo 4\n",
      "  - Etapa: Etapa 10 of Fluxo 4\n",
      "Fluxo: Fluxo 5\n",
      "obj.references: dict_keys(['hasEtapas'])\n",
      "  - Etapa: Etapa 1 of Fluxo 5\n",
      "  - Etapa: Etapa 2 of Fluxo 5\n",
      "  - Etapa: Etapa 3 of Fluxo 5\n",
      "  - Etapa: Etapa 4 of Fluxo 5\n",
      "  - Etapa: Etapa 5 of Fluxo 5\n",
      "  - Etapa: Etapa 6 of Fluxo 5\n",
      "  - Etapa: Etapa 7 of Fluxo 5\n",
      "  - Etapa: Etapa 8 of Fluxo 5\n",
      "  - Etapa: Etapa 9 of Fluxo 5\n",
      "  - Etapa: Etapa 10 of Fluxo 5\n"
     ]
    }
   ],
   "source": [
    "#Retrieve \"Etapas\" from a given \"Fluxo\"\n",
    "from weaviate.classes.query import QueryReference\n",
    "\n",
    "query_result = fluxo.query.fetch_objects(\n",
    "    return_properties=[\"name\"],\n",
    "    return_references=QueryReference(\n",
    "        link_on=\"hasEtapas\",\n",
    "        return_properties=[\"name\"]\n",
    "    ),\n",
    "    limit=10\n",
    ")\n",
    "for obj in query_result.objects:\n",
    "    print(f\"Fluxo: {obj.properties['name']}\")\n",
    "    print(f\"obj.references: {obj.references.keys()}\")\n",
    "    if \"hasEtapas\" in obj.references:\n",
    "        for etapa in obj.references[\"hasEtapas\"].objects:\n",
    "            print(f\"  - Etapa: {etapa.properties['name']}\")\n",
    "    else:\n",
    "        print(\"No 'hasEtapas' reference found for this Fluxo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidade: Empresa 2\n",
      "  - Pasta: Pasta 1|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 2|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 3|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 4|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 5|- Ficheiros: {0}\n",
      "Entidade: Empresa 1\n",
      "  - Pasta: Pasta 1|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 2|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 3|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 4|- Ficheiros: {0}\n",
      "  - Pasta: Pasta 5|- Ficheiros: {0}\n"
     ]
    }
   ],
   "source": [
    "query_result = entidade.query.fetch_objects(\n",
    "    return_properties=[\"name\"],\n",
    "    return_references=QueryReference(\n",
    "        link_on=\"hasPastas\",\n",
    "        return_properties=[\"name\"],\n",
    "        return_references=QueryReference(\n",
    "            link_on=\"hasFicheiros\",\n",
    "            return_properties=[\"nome\"],\n",
    "            return_references=QueryReference(\n",
    "                link_on=\"belongsToMetadados\",\n",
    "                return_properties=[\"dados\"]\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    limit=10\n",
    ")\n",
    "for obj in query_result.objects:\n",
    "    print(f\"Entidade: {obj.properties['name']}\")\n",
    "    for pasta in obj.references[\"hasPastas\"].objects:\n",
    "        count_ficheiros = 0\n",
    "        print(f\"  - Pasta: {pasta.properties['name'][0:7]}\", end=\"|\")\n",
    "        print(f\"- Ficheiros:\", {count_ficheiros})\n",
    "        for ficheiro in pasta.references[\"hasFicheiros\"].objects:\n",
    "            count_ficheiros += 1\n",
    "            if \"belongsToMetadados\" in ficheiro.references:\n",
    "                for meta in ficheiro.references[\"belongsToMetadados\"].objects:\n",
    "                    print(f\"      - Metadados: {meta.properties['dados']}\")\n",
    "            # else:\n",
    "            #     print(\"      - No 'belongsToMetadados' reference found for this Documento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pasta] Pasta 2 of Empresa 1 (Score: 0.20000000298023224)\n",
      "[Fluxo] Fluxo 3 (Score: 0.20000000298023224)\n",
      "[Etapa] Etapa 8 of Fluxo 1 (Score: 0.20000000298023224)\n",
      "[Metadados] None (Score: 0.20000000298023224)\n",
      "[Metadados] None (Score: 0.20000000298023224)\n",
      "[Metadados] None (Score: 0.20000000298023224)\n",
      "[Metadados] None (Score: 0.20000000298023224)\n",
      "[Metadados] None (Score: 0.20000000298023224)\n",
      "[Ficheiro] None (Score: 0.20000000298023224)\n",
      "[Ficheiro] None (Score: 0.20000000298023224)\n",
      "[Ficheiro] None (Score: 0.20000000298023224)\n",
      "[Ficheiro] None (Score: 0.20000000298023224)\n",
      "[Ficheiro] None (Score: 0.20000000298023224)\n",
      "[Entidade] Empresa 2 (Score: 0.20000000298023224)\n",
      "[Etapa] Etapa 9 of Fluxo 1 (Score: 0.19135645031929016)\n",
      "[Etapa] Etapa 7 of Fluxo 1 (Score: 0.18554548919200897)\n",
      "[Fluxo] Fluxo 2 (Score: 0.18322160840034485)\n",
      "[Etapa] Etapa 9 of Fluxo 4 (Score: 0.18066634237766266)\n",
      "[Etapa] Etapa 9 of Fluxo 2 (Score: 0.17712409794330597)\n",
      "[Pasta] Pasta 2 of Empresa 2 (Score: 0.14277087152004242)\n",
      "[Pasta] Pasta 3 of Empresa 2 (Score: 0.12948068976402283)\n",
      "[Pasta] Pasta 4 of Empresa 2 (Score: 0.1247502788901329)\n",
      "[Pasta] Pasta 3 of Empresa 1 (Score: 0.11673355102539062)\n",
      "[Fluxo] Fluxo 4 (Score: 0.06163882836699486)\n",
      "[Fluxo] Fluxo 5 (Score: 0.03389469534158707)\n",
      "[Fluxo] Fluxo 1 (Score: 0.0)\n",
      "[Entidade] Empresa 1 (Score: 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Global Semantic Search\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "query_text = \"Find documents about contract approvals\"\n",
    "\n",
    "# Search across multiple collections\n",
    "results = []\n",
    "for collection_name in [\"Pasta\", \"Fluxo\", \"Etapa\", \"Metadados\", \"Ficheiro\", \"Entidade\"]:\n",
    "    collection = client.collections.get(collection_name)\n",
    "\n",
    "    search_results = collection.query.hybrid(\n",
    "        query=query_text,  \n",
    "        alpha=0.2,  # Set to 0.2 for balanced results\n",
    "        return_properties=[\"name\"],  # Return these properties\n",
    "        return_metadata=MetadataQuery(score=True),  # Return relevance score\n",
    "        limit=5,  # Get top 5 results per collection\n",
    "    )\n",
    "\n",
    "    # Store results with class name\n",
    "    for obj in search_results.objects:\n",
    "        results.append({\n",
    "            \"class\": collection_name,\n",
    "            \"name\": obj.properties[\"name\"],\n",
    "            \"score\": obj.metadata.score,\n",
    "        })\n",
    "\n",
    "# Sort results by relevance score (higher is better)\n",
    "results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"[{result['class']}] {result['name']} (Score: {result['score']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
