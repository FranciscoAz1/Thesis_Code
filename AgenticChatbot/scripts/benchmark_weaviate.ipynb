{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutConfig not available in this weaviate version; using default client timeouts.\n",
      "Server modules detected: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'AdministrativeDocuments' ready (text2vec-transformers).\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Sets up Weaviate collection using text2vec-transformers (external inference container).\n",
    "# Ensure docker-compose is up with services: weaviate + t2v-transformers.\n",
    "#   ENABLE_MODULES=text2vec-transformers,generative-ollama\n",
    "#   DEFAULT_VECTORIZER_MODULE=text2vec-transformers\n",
    "#   TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080\n",
    "# Optional: generative-ollama (Ollama running on host for qwen2m:latest)\n",
    "# Timeout tuning: Some weaviate client versions expose TimeoutConfig; if not, we fall back.\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization# List user-site Jupyter packages\n",
    "\n",
    "# Attempt optional TimeoutConfig (newer weaviate client). If missing, continue with defaults.\n",
    "try:\n",
    "    from weaviate.connect import TimeoutConfig  # may not exist in older versions\n",
    "    TIMEOUTS = TimeoutConfig(init=60, query=180, insert=120)\n",
    "    client = weaviate.connect_to_local(timeout_config=TIMEOUTS)\n",
    "    print(\"Custom timeouts applied:\", TIMEOUTS)\n",
    "except ImportError:\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig not available in this weaviate version; using default client timeouts.\")\n",
    "except TypeError:\n",
    "    # Signature mismatch (older version). Reconnect without custom timeouts.\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig signature unsupported; using default timeouts.\")\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # must match the model available to Ollama on host\n",
    "DATASET_NAME = \"AdministrativeDocuments\"\n",
    "\n",
    "# Diagnostics\n",
    "try:\n",
    "    meta = client.get_meta()\n",
    "    print(\"Server modules detected:\", list(meta.get(\"modules\", {}).keys()))\n",
    "except Exception as e:\n",
    "    print(\"Meta fetch failed:\", e)\n",
    "\n",
    "# Recreate collection for a clean slate\n",
    "try:\n",
    "    client.collections.delete(DATASET_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "api_endpoint = \"http://host.docker.internal:11434\"  # Ollama on host\n",
    "\n",
    "client.collections.create(\n",
    "    DATASET_NAME,\n",
    "    description=\"Administrative documents repository\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.LOWERCASE),\n",
    "        Property(name=\"file_path\", data_type=DataType.TEXT)\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_transformers(\n",
    "            name=\"text_vector\",\n",
    "            source_properties=[\"text\"],\n",
    "            pooling_strategy=\"masked_mean\",\n",
    "        )\n",
    "    ],\n",
    "    generative_config=Configure.Generative.ollama(        \n",
    "        api_endpoint=api_endpoint,\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "assert client.collections.exists(DATASET_NAME)\n",
    "print(f\"Collection '{DATASET_NAME}' ready (text2vec-transformers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 1000 docs in 17.63s\n",
      "Start RSS: 664.74 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018a182c94884f4ea9d5fa786c9ea8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 / 1000 docs in 10.53s (95.00 docs/s)\n",
      "End RSS: 584.45 MB (Œî -80.29 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "from docx import Document\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\\documentos_gerados\"\n",
    "WORKING_DIR = r\"C:\\Users\\Francisco Azeredo\\Downloads\\gerador_documentos_gpt_azure (1)\\gerador_documentos_gpt_azure\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "        if suffix == \".docx\":\n",
    "            # Read .docx files using python-docx\n",
    "            doc = Document(path)\n",
    "            text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    text.append(paragraph.text.strip())\n",
    "            return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\", \".docx\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Œî {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(DATASET_NAME)\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Query & QA Evaluation - Best Metrics Only\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Optimized to focus on the most meaningful metrics for RAG evaluation.\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration - Using Relative Paths --------\n",
    "# Updated to use relative paths after reorganization\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Add parent to path for imports\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "QA_JSON_PATH = str(PROJECT_ROOT / \"datasets\" / \"qa_dataset.json\")\n",
    "OUTPUT_CSV_PATH = str(PROJECT_ROOT / \"results\")  # set to None to skip saving\n",
    "TOP_K = 3            # lower to reduce vector fetch time\n",
    "MAX_Q = None         # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True  # semantic metrics cost\n",
    "PER_QUERY_DEADLINE = 60.0  # seconds, must be < client.query timeout\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF = 5.0  # seconds added each retry\n",
    "PROMPT_PREFIX = \"Answer briefly: \"  # keep prompt short -> faster generation\n",
    "COT_PROMPT = True  # chain-of-thought prompting (slower, may improve complex Qs)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"‚úì Paths configured:\")\n",
    "print(f\"  Q&A Dataset: {QA_JSON_PATH}\")\n",
    "print(f\"  Output Folder: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  Dataset exists: {Path(QA_JSON_PATH).exists()}\")\n",
    "print(f\"  Output folder exists: {Path(OUTPUT_CSV_PATH).exists()}\")\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "def tokenize_pt(s: str):\n",
    "    return [t.lower() for t in TOKEN_SPLIT_RE.split(s) if t.strip()]\n",
    "\n",
    "def token_recall(answer: str, gold: str) -> float:\n",
    "    at = set(tokenize_pt(answer))\n",
    "    gt = set(tokenize_pt(gold))\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "    return len(at & gt) / len(gt)\n",
    "\n",
    "def compute_rouge1(hyp: str, ref: str):\n",
    "    if not hyp.strip() or not ref.strip():\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "    try:\n",
    "        return _lazy_rouge().get_scores(hyp, ref)[0]\n",
    "    except:\n",
    "        return {\"rouge-1\": {\"f\": 0.0, \"p\": 0.0, \"r\": 0.0}}\n",
    "\n",
    "def compute_bleu(hyp: str, ref: str) -> float:\n",
    "    h_toks = tokenize_pt(hyp)\n",
    "    r_toks = tokenize_pt(ref)\n",
    "    if not h_toks or not r_toks:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return sentence_bleu([r_toks], h_toks, smoothing_function=_SMOOTH)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_bert_similarity(hyp: str, ref: str) -> float:\n",
    "    if not USE_BERT_SIM or not hyp.strip() or not ref.strip():\n",
    "        return 0.0\n",
    "    model = _lazy_bert()\n",
    "    emb = model.encode([hyp, ref])\n",
    "    return float(np.dot(emb[0], emb[1]) / (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1])))\n",
    "\n",
    "def best_sentence_overlap(answer: str, gold: str) -> float:\n",
    "    sents_a = sent_tokenize(answer, language=\"portuguese\")\n",
    "    sents_g = sent_tokenize(gold, language=\"portuguese\")\n",
    "    best = 0.0\n",
    "    for sa in sents_a:\n",
    "        for sg in sents_g:\n",
    "            tr = token_recall(sa, sg)\n",
    "            if tr > best:\n",
    "                best = tr\n",
    "    return best\n",
    "\n",
    "# Load QA\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_JSON_PATH):\n",
    "    with open(QA_JSON_PATH, encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "        for item in qa_data:\n",
    "            if \"pergunta\" in item and \"resposta\" in item:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": item[\"pergunta\"].strip(),\n",
    "                    \"gold_answer\": item[\"resposta\"].strip(),\n",
    "                    \"context\": item.get(\"contexto\",\"\").strip(),\n",
    "                    \"file\": item.get(\"arquivo\",\"\").strip()\n",
    "                })\n",
    "else:\n",
    "    print(\"QA JSON not found. Check QA_JSON_PATH.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    random.shuffle(qa_pairs)\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} Q&A pairs.\")\n",
    "\n",
    "rows = []\n",
    "latencies = []\n",
    "correct_retrievals = 0\n",
    "cot_phrase = \" Let's think step by step.\" if COT_PROMPT else \"\"\n",
    "\n",
    "for idx, pair in enumerate(tqdm(qa_pairs, desc=\"Evaluating (Best Metrics)\")):\n",
    "    q = pair[\"question\"]\n",
    "    gold_answer = pair[\"gold_answer\"]\n",
    "    gold_context = pair[\"context\"]\n",
    "    gold_file = pair[\"file\"]\n",
    "    \n",
    "    start_t = time.perf_counter()\n",
    "    try:\n",
    "        response_obj = rag.query(PROMPT_PREFIX + q + cot_phrase, param={\"limit\": TOP_K})\n",
    "        \n",
    "        if not response_obj or not hasattr(response_obj, \"answer\"):\n",
    "            print(f\"Q#{idx+1}: No valid response. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        answer_text = response_obj.answer.strip() if response_obj.answer else \"\"\n",
    "        retrieved_context = \" \".join([ctx.get(\"content\", \"\") for ctx in response_obj.context]) if hasattr(response_obj, \"context\") else \"\"\n",
    "        \n",
    "        # Retrieval metrics\n",
    "        retrieved_files = [ctx.get(\"file_path\", \"\") for ctx in response_obj.context] if hasattr(response_obj, \"context\") else []\n",
    "        rank = retrieved_files.index(gold_file) if gold_file in retrieved_files else -1\n",
    "        if rank >= 0:\n",
    "            correct_retrievals += 1\n",
    "        \n",
    "        # Answer quality (best metrics)\n",
    "        exact = 1.0 if normalize_text(answer_text) == normalize_text(gold_answer) else 0.0\n",
    "        substring = 1.0 if gold_answer.lower() in answer_text.lower() else 0.0\n",
    "        tok_recall = token_recall(answer_text, gold_answer)\n",
    "        rouge1_scores = compute_rouge1(answer_text, gold_answer)\n",
    "        rouge1_f = rouge1_scores[\"rouge-1\"][\"f\"]\n",
    "        bert_cos = compute_bert_similarity(answer_text, gold_answer) if USE_BERT_SIM else 0.0\n",
    "        \n",
    "        # Context quality\n",
    "        context_token_recall = token_recall(retrieved_context, gold_context) if gold_context else 0.0\n",
    "        context_rouge1 = compute_rouge1(retrieved_context, gold_context) if gold_context else {\"rouge-1\": {\"f\": 0.0}}\n",
    "        context_bert = compute_bert_similarity(retrieved_context, gold_context) if USE_BERT_SIM and gold_context else 0.0\n",
    "        \n",
    "        lat = time.perf_counter() - start_t\n",
    "        latencies.append(lat)\n",
    "        \n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"generated_answer\": answer_text,\n",
    "            \"gold_file\": gold_file,\n",
    "            \"retrieved_files\": \"|\".join(retrieved_files),\n",
    "            \"retrieval_rank\": rank,\n",
    "            \"exact\": exact,\n",
    "            \"substring\": substring,\n",
    "            \"token_recall\": tok_recall,\n",
    "            \"rouge1_f\": rouge1_f,\n",
    "            \"bert_cos\": bert_cos,\n",
    "            \"context_token_recall\": context_token_recall,\n",
    "            \"context_rouge1_f\": context_rouge1[\"rouge-1\"][\"f\"],\n",
    "            \"context_bert_cos\": context_bert,\n",
    "            \"latency_s\": lat\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Q#{idx+1} error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Results summary\n",
    "if rows:\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    \n",
    "    # Report best metrics only\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAG EVALUATION RESULTS - BEST METRICS ONLY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RETRIEVAL PERFORMANCE:\")\n",
    "    print(f\"  Document Retrieval Accuracy: {correct_retrievals}/{len(qa_pairs)} = {correct_retrievals/len(qa_pairs):.2%}\")\n",
    "    print(f\"  Average Retrieval Rank: {_avg('retrieval_rank'):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìù ANSWER QUALITY:\")\n",
    "    print(f\"  Exact Match: {_avg('exact'):.2%}\")\n",
    "    print(f\"  Substring Match: {_avg('substring'):.2%}\")\n",
    "    print(f\"  Token Recall: {_avg('token_recall'):.3f}\")\n",
    "    print(f\"  ROUGE-1 F1: {_avg('rouge1_f'):.3f}\")\n",
    "    if 'bert_cos' in rows[0]:\n",
    "        print(f\"  BERT Similarity: {_avg('bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüîç CONTEXT QUALITY:\")\n",
    "    if 'context_token_recall' in rows[0]:\n",
    "        print(f\"  Context Token Recall: {_avg('context_token_recall'):.3f}\")\n",
    "    if 'context_rouge1_f' in rows[0]:\n",
    "        print(f\"  Context ROUGE-1 F1: {_avg('context_rouge1_f'):.3f}\")\n",
    "    if 'context_bert_cos' in rows[0]:\n",
    "        print(f\"  Context BERT Similarity: {_avg('context_bert_cos'):.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95 = statistics.quantiles(latencies, n=20)[18] if len(latencies)>=20 else max(latencies)\n",
    "    print(f\"  Average Latency: {avg_lat*1000:.1f}ms\")\n",
    "    print(f\"  95th Percentile Latency: {p95*1000:.1f}ms\")\n",
    "    print(f\"  Questions per Second: {1/avg_lat:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        mode = \"AgentCoT\" if COT_PROMPT else \"Agent\"\n",
    "        n = len(rows)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"{mode}_{n}.csv\")\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"\\n‚úì Results saved to: {out_file}\")\n",
    "else:\n",
    "    print(\"\\nNo valid results to report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connections before running benchmark\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"üîç Testing connections...\")\n",
    "\n",
    "# Test Ollama\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(f\"‚úÖ Ollama is running with {len(models)} models\")\n",
    "        qwen_available = any('qwen2.5' in model.get('name', '') for model in models)\n",
    "        if qwen_available:\n",
    "            print(\"‚úÖ qwen2.5 model is available\")\n",
    "        else:\n",
    "            print(\"‚ùå qwen2.5 model not found - run: ollama pull qwen2.5:latest\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama responded with status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection failed: {e}\")\n",
    "    print(\"Run: ollama serve\")\n",
    "\n",
    "# Test Weaviate\n",
    "try:\n",
    "    if 'client' in globals():\n",
    "        meta = client.get_meta()\n",
    "        print(f\"‚úÖ Weaviate is running, modules: {list(meta.get('modules', {}).keys())}\")\n",
    "        \n",
    "        # Test collection exists\n",
    "        if client.collections.exists(\"Dataset\"):\n",
    "            collection = client.collections.get(\"Dataset\")\n",
    "            aggregate = collection.aggregate.over_all(total_count=True)\n",
    "            count = aggregate.total_count\n",
    "            print(f\"‚úÖ Dataset collection has {count} documents\")\n",
    "            \n",
    "            # Simple test query (no generation)\n",
    "            try:\n",
    "                test_result = collection.query.near_text(\n",
    "                    query=\"test\", \n",
    "                    limit=1,\n",
    "                    return_metadata=['distance']\n",
    "                )\n",
    "                print(\"‚úÖ Vector search is working\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Vector search failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå Dataset collection not found\")\n",
    "    else:\n",
    "        print(\"‚ùå Weaviate client not initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Weaviate connection failed: {e}\")\n",
    "\n",
    "print(\"\\nüéØ If all tests pass, you can proceed with the benchmark.\")\n",
    "print(\"üéØ If tests fail, fix the issues before running the evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
