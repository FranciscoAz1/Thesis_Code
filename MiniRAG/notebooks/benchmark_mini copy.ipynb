{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_bart_lexrank\\\\vdb_entities.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_bart_lexrank\\\\vdb_entities_name.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_bart_lexrank\\\\vdb_relationships.json'} 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_bart_lexrank\\\\vdb_chunks.json'} 0 data\n",
      "INFO:minirag:Loaded document status storage with 0 records\n",
      "INFO:minirag.summarization.bart_summarizer:Loaded BART model: facebook/bart-large-cnn on cuda\n",
      "INFO:minirag:BART summarizer initialized for entity description summarization\n",
      "INFO:minirag:LexRank summarizer initialized for entity extraction (ratio: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Loads embedding model, builds embedding_func, and instantiates a MiniRAG object.\n",
    "# Does NOT ingest documents. Use the next cell to index.\n",
    "\n",
    "import os, torch, sys\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag.llm.openai import openai_complete\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before running this cell.\")\n",
    "\n",
    "sys.path.append(r'C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag')\n",
    "\n",
    "# Core configuration (shared by later cells)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\notebooks\\\\storage_bart_lexrank\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # set to None if no local Ollama model\n",
    "LOG_LEVEL = \"INFO\"  # Changed from \"CRITICAL\" to see initialization messages\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func = lambda texts: hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model),\n",
    ")\n",
    "rag = MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "    llm_model_name=LLM_MODEL_NAME,\n",
    "    embedding_func=_embedding_func,\n",
    "    log_level=LOG_LEVEL,\n",
    "    suppress_httpx_logging=True,\n",
    "    summarize_before_chunking=True,\n",
    "    document_summary_ratio=1,\n",
    "    use_bart_entity_extraction=True,\n",
    "    key_sentences_lexrank=True,\n",
    "    lexrank_ratio=1,\n",
    "    chunk_token_size=200,\n",
    "    chunk_overlap_token_size=5,\n",
    ")\n",
    "print(\"RAG initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e23b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Warming up Ollama model: qwen2.5:latest\n",
      "âœ… Ollama ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Warm up Ollama (Optional)\n",
    "# ----------------------------------\n",
    "# Ensures the Ollama model is loaded before Weaviate calls it.\n",
    "\n",
    "# You can rerun this cell anytime after starting Ollama or changing models.\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configure warm-up behavior\n",
    "AUTO_WARMUP_OLLAMA = True\n",
    "WARMUP_TIMEOUT = 600          # seconds to wait for model to be ready\n",
    "WARMUP_POLL_INTERVAL = 5      # seconds between checks\n",
    "WARMUP_API_URL = \"http://localhost:11434/api/generate\"  # Ollama generate endpoint\n",
    "\n",
    "def warm_up_ollama(model: str | None = None, timeout_s: int = WARMUP_TIMEOUT, poll_interval: int = WARMUP_POLL_INTERVAL, prompt: str = \"ping\", api_url: str = WARMUP_API_URL) -> bool:\n",
    "    \"\"\"Ping Ollama generate API until the model is ready or timeout expires.\n",
    "    Returns True when ready, False if timed out.\"\"\"\n",
    "    mdl = model or globals().get(\"LLM_MODEL_NAME\", \"qwen2.5:latest\")\n",
    "    print(f\"ðŸ”§ Warming up Ollama model: {mdl}\")\n",
    "    start = time.perf_counter()\n",
    "    last_err = None\n",
    "    while (time.perf_counter() - start) < timeout_s:\n",
    "        try:\n",
    "            resp = requests.post(api_url, json={\"model\": mdl, \"prompt\": prompt, \"stream\": False}, timeout=3000)\n",
    "            if resp.status_code == 200:\n",
    "                try:\n",
    "                    data = resp.json()\n",
    "                except Exception:\n",
    "                    data = {}\n",
    "                text = data.get(\"response\") or data.get(\"message\") or \"\"\n",
    "                print(\"âœ… Ollama ready.\")\n",
    "                return True\n",
    "            else:\n",
    "                last_err = f\"HTTP {resp.status_code}: {resp.text[:200]}\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "        print(f\"â€¦waiting for Ollama to load model (poll {poll_interval}s). Last error: {last_err}\")\n",
    "        time.sleep(poll_interval)\n",
    "    print(f\"â±ï¸ Warm-up timed out after {timeout_s}s. Last error: {last_err}\")\n",
    "    return False\n",
    "\n",
    "# Auto warm-up when this cell runs\n",
    "if AUTO_WARMUP_OLLAMA:\n",
    "    warm_up_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 442 docs in 0.07s\n",
      "Start RSS: 948.68 MB\n",
      "Indexing with ainsert() ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8451eca296b461e9fc38ee100a328d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-8b78dad33a42bc7933755eccb74875d2 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-8b78dad33a42bc7933755eccb74875d2. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-8b78dad33a42bc7933755eccb74875d2 summarized: 771 â†’ 359 chars (53.4% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 2 chunks, 2 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:minirag:Didn't extract any relationships, maybe your LLM is not working\n",
      "INFO:minirag:Writing graph with 2 nodes, 0 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-68b13480ea1b74f8768581cae0d17cbc in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-68b13480ea1b74f8768581cae0d17cbc. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-68b13480ea1b74f8768581cae0d17cbc summarized: 599 â†’ 360 chars (39.9% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 3 chunks, 10 entities(duplicated), 3 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 10 vectors to entities\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "INFO:minirag:Inserting 10 vectors to entities\n",
      "INFO:minirag:Inserting 10 vectors to entities_name\n",
      "INFO:minirag:Inserting 3 vectors to relationships\n",
      "INFO:minirag:Writing graph with 11 nodes, 3 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-5cb5b75e5b5a346a58a1c8c2474d0a4a in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-5cb5b75e5b5a346a58a1c8c2474d0a4a. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-5cb5b75e5b5a346a58a1c8c2474d0a4a summarized: 508 â†’ 334 chars (34.3% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 4 chunks, 18 entities(duplicated), 9 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 17 vectors to entities\n",
      "INFO:minirag:Inserting 17 vectors to entities\n",
      "INFO:minirag:Inserting 17 vectors to entities_name\n",
      "INFO:minirag:Inserting 9 vectors to relationships\n",
      "INFO:minirag:Writing graph with 29 nodes, 11 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-e7e1442c7dd2de3345678c4a1be7deba in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-e7e1442c7dd2de3345678c4a1be7deba. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-e7e1442c7dd2de3345678c4a1be7deba summarized: 15316 â†’ 404 chars (97.4% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 20 chunks, 98 entities(duplicated), 27 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 91 vectors to entities\n",
      "INFO:minirag:Inserting 91 vectors to entities\n",
      "INFO:minirag:Inserting 91 vectors to entities_name\n",
      "INFO:minirag:Inserting 27 vectors to relationships\n",
      "INFO:minirag:Writing graph with 115 nodes, 38 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-f83c2db10795f4aac1c41599adf0a127 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-f83c2db10795f4aac1c41599adf0a127. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-f83c2db10795f4aac1c41599adf0a127 summarized: 861 â†’ 343 chars (60.2% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 22 chunks, 110 entities(duplicated), 60 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 88 vectors to entities\n",
      "INFO:minirag:Inserting 88 vectors to entities\n",
      "INFO:minirag:Inserting 88 vectors to entities_name\n",
      "INFO:minirag:Inserting 58 vectors to relationships\n",
      "INFO:minirag:Writing graph with 166 nodes, 93 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-25949b1be91fdd89af34675367889766 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-25949b1be91fdd89af34675367889766. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-25949b1be91fdd89af34675367889766 summarized: 2733 â†’ 294 chars (89.2% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 27 chunks, 136 entities(duplicated), 47 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 112 vectors to entities\n",
      "INFO:minirag:Inserting 112 vectors to entities\n",
      "INFO:minirag:Inserting 112 vectors to entities_name\n",
      "INFO:minirag:Inserting 47 vectors to relationships\n",
      "INFO:minirag:Writing graph with 229 nodes, 131 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-a20ef9c90c581f8ce8da5881198cf498 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-a20ef9c90c581f8ce8da5881198cf498. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-a20ef9c90c581f8ce8da5881198cf498 summarized: 778 â†’ 262 chars (66.3% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 29 chunks, 153 entities(duplicated), 45 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 122 vectors to entities\n",
      "INFO:minirag:Inserting 122 vectors to entities\n",
      "INFO:minirag:Inserting 122 vectors to entities_name\n",
      "INFO:minirag:Inserting 43 vectors to relationships\n",
      "INFO:minirag:Writing graph with 298 nodes, 160 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-28a8002785187562062f9468957eee61 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-28a8002785187562062f9468957eee61. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-28a8002785187562062f9468957eee61 summarized: 409 â†’ 289 chars (29.3% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 30 chunks, 141 entities(duplicated), 58 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 108 vectors to entities\n",
      "INFO:minirag:Inserting 108 vectors to entities\n",
      "INFO:minirag:Inserting 108 vectors to entities_name\n",
      "INFO:minirag:Inserting 57 vectors to relationships\n",
      "INFO:minirag:Writing graph with 354 nodes, 196 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-00fa03493d7030d4bc825aede0beae58 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-00fa03493d7030d4bc825aede0beae58. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-00fa03493d7030d4bc825aede0beae58 summarized: 948 â†’ 385 chars (59.4% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 32 chunks, 159 entities(duplicated), 59 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 118 vectors to entities\n",
      "INFO:minirag:Inserting 118 vectors to entities\n",
      "INFO:minirag:Inserting 118 vectors to entities_name\n",
      "INFO:minirag:Inserting 56 vectors to relationships\n",
      "INFO:minirag:Writing graph with 413 nodes, 231 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-86b91a781aaee4aec8ceee5f086856f3 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-86b91a781aaee4aec8ceee5f086856f3. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-86b91a781aaee4aec8ceee5f086856f3 summarized: 726 â†’ 335 chars (53.9% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 34 chunks, 155 entities(duplicated), 38 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 118 vectors to entities\n",
      "INFO:minirag:Inserting 118 vectors to entities\n",
      "INFO:minirag:Inserting 118 vectors to entities_name\n",
      "INFO:minirag:Inserting 37 vectors to relationships\n",
      "INFO:minirag:Writing graph with 470 nodes, 252 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-cb1169c5aefd74021c8e93cdf66d9221 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-cb1169c5aefd74021c8e93cdf66d9221. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-cb1169c5aefd74021c8e93cdf66d9221 summarized: 1354 â†’ 284 chars (79.0% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processed 36 chunks, 164 entities(duplicated), 61 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 125 vectors to entities\n",
      "INFO:minirag:Inserting 125 vectors to entities\n",
      "INFO:minirag:Inserting 125 vectors to entities_name\n",
      "INFO:minirag:Inserting 57 vectors to relationships\n",
      "INFO:minirag:Writing graph with 524 nodes, 289 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-c283e17589a754bb347cf3e7bd8b857d in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-c283e17589a754bb347cf3e7bd8b857d. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-c283e17589a754bb347cf3e7bd8b857d summarized: 9332 â†’ 367 chars (96.1% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 47 chunks, 244 entities(duplicated), 92 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 168 vectors to entities\n",
      "INFO:minirag:Inserting 168 vectors to entities\n",
      "INFO:minirag:Inserting 168 vectors to entities_name\n",
      "INFO:minirag:Inserting 90 vectors to relationships\n",
      "INFO:minirag:Writing graph with 625 nodes, 336 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-4c79c70ad719166c0813db6f68ba8dfd in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-4c79c70ad719166c0813db6f68ba8dfd. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-4c79c70ad719166c0813db6f68ba8dfd summarized: 514 â†’ 273 chars (46.9% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‡ Processed 48 chunks, 256 entities(duplicated), 96 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 172 vectors to entities\n",
      "INFO:minirag:Inserting 172 vectors to entities\n",
      "INFO:minirag:Inserting 172 vectors to entities_name\n",
      "INFO:minirag:Inserting 91 vectors to relationships\n",
      "INFO:minirag:Writing graph with 719 nodes, 396 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-addb66bd145b4c2c70d4a7c635fa9551 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-addb66bd145b4c2c70d4a7c635fa9551. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-addb66bd145b4c2c70d4a7c635fa9551 summarized: 612 â†’ 325 chars (46.9% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 49 chunks, 216 entities(duplicated), 107 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 142 vectors to entities\n",
      "INFO:minirag:Inserting 142 vectors to entities\n",
      "INFO:minirag:Inserting 142 vectors to entities_name\n",
      "INFO:minirag:Inserting 98 vectors to relationships\n",
      "INFO:minirag:Writing graph with 778 nodes, 453 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-c8d89524d032cc07376064809227e195 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-c8d89524d032cc07376064809227e195. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-c8d89524d032cc07376064809227e195 summarized: 4828 â†’ 333 chars (93.1% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processed 56 chunks, 299 entities(duplicated), 96 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 207 vectors to entities\n",
      "INFO:minirag:Inserting 207 vectors to entities\n",
      "INFO:minirag:Inserting 207 vectors to entities_name\n",
      "INFO:minirag:Inserting 92 vectors to relationships\n",
      "INFO:minirag:Writing graph with 888 nodes, 518 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-d8d71bca6bcbcda860e80d60d5d35a1d in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-d8d71bca6bcbcda860e80d60d5d35a1d. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-d8d71bca6bcbcda860e80d60d5d35a1d summarized: 431 â†’ 215 chars (50.1% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 57 chunks, 280 entities(duplicated), 96 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 189 vectors to entities\n",
      "INFO:minirag:Inserting 189 vectors to entities\n",
      "INFO:minirag:Inserting 189 vectors to entities_name\n",
      "INFO:minirag:Inserting 93 vectors to relationships\n",
      "INFO:minirag:Writing graph with 977 nodes, 582 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-c4bac505a7a4784fc1b688e2b8871c66 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-c4bac505a7a4784fc1b688e2b8871c66. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-c4bac505a7a4784fc1b688e2b8871c66 summarized: 827 â†’ 354 chars (57.2% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 59 chunks, 300 entities(duplicated), 88 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 208 vectors to entities\n",
      "INFO:minirag:Inserting 208 vectors to entities\n",
      "INFO:minirag:Inserting 208 vectors to entities_name\n",
      "INFO:minirag:Inserting 84 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1075 nodes, 626 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-fe507a84af004f6890da1d09c0cd6ba6 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-fe507a84af004f6890da1d09c0cd6ba6. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-fe507a84af004f6890da1d09c0cd6ba6 summarized: 144 â†’ 145 chars (-0.7% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 60 chunks, 271 entities(duplicated), 63 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 175 vectors to entities\n",
      "INFO:minirag:Inserting 175 vectors to entities\n",
      "INFO:minirag:Inserting 175 vectors to entities_name\n",
      "INFO:minirag:Inserting 59 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1137 nodes, 654 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-afd4cf6e494381b30bf09739eae250b7 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-afd4cf6e494381b30bf09739eae250b7. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-afd4cf6e494381b30bf09739eae250b7 summarized: 613 â†’ 305 chars (50.2% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 61 chunks, 326 entities(duplicated), 76 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 223 vectors to entities\n",
      "INFO:minirag:Inserting 223 vectors to entities\n",
      "INFO:minirag:Inserting 223 vectors to entities_name\n",
      "INFO:minirag:Inserting 70 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1219 nodes, 696 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-c13f597cc7d58f32274ed3bba87c5f2f in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-c13f597cc7d58f32274ed3bba87c5f2f. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-c13f597cc7d58f32274ed3bba87c5f2f summarized: 712 â†’ 328 chars (53.9% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 62 chunks, 300 entities(duplicated), 84 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 209 vectors to entities\n",
      "INFO:minirag:Inserting 209 vectors to entities\n",
      "INFO:minirag:Inserting 209 vectors to entities_name\n",
      "INFO:minirag:Inserting 78 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1312 nodes, 743 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-51500f527ced6ced90102d7eac6e1375 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-51500f527ced6ced90102d7eac6e1375. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-51500f527ced6ced90102d7eac6e1375 summarized: 144 â†’ 145 chars (-0.7% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 63 chunks, 323 entities(duplicated), 80 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 215 vectors to entities\n",
      "INFO:minirag:Inserting 215 vectors to entities\n",
      "INFO:minirag:Inserting 215 vectors to entities_name\n",
      "INFO:minirag:Inserting 74 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1387 nodes, 781 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-e22918849bd19caf74c561efbf6a615e in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-e22918849bd19caf74c561efbf6a615e. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-e22918849bd19caf74c561efbf6a615e summarized: 676 â†’ 355 chars (47.5% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 64 chunks, 346 entities(duplicated), 98 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 234 vectors to entities\n",
      "INFO:minirag:Inserting 234 vectors to entities\n",
      "INFO:minirag:Inserting 234 vectors to entities_name\n",
      "INFO:minirag:Inserting 88 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1464 nodes, 830 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-9a5e3991bf8576ca2b09c937e3017f03 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-9a5e3991bf8576ca2b09c937e3017f03. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-9a5e3991bf8576ca2b09c937e3017f03 summarized: 5395 â†’ 375 chars (93.0% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 71 chunks, 378 entities(duplicated), 106 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 255 vectors to entities\n",
      "INFO:minirag:Inserting 255 vectors to entities\n",
      "INFO:minirag:Inserting 255 vectors to entities_name\n",
      "INFO:minirag:Inserting 100 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1580 nodes, 878 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-608802d26630444a57c3070cc7f581a2 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-608802d26630444a57c3070cc7f581a2. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-608802d26630444a57c3070cc7f581a2 summarized: 523 â†’ 329 chars (37.1% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 72 chunks, 386 entities(duplicated), 119 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 254 vectors to entities\n",
      "INFO:minirag:Inserting 254 vectors to entities\n",
      "INFO:minirag:Inserting 254 vectors to entities_name\n",
      "INFO:minirag:Inserting 115 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1694 nodes, 946 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-182889c8ae3097757853b5e162ce93b9 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-182889c8ae3097757853b5e162ce93b9. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-182889c8ae3097757853b5e162ce93b9 summarized: 684 â†’ 290 chars (57.6% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 73 chunks, 404 entities(duplicated), 144 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 271 vectors to entities\n",
      "INFO:minirag:Inserting 271 vectors to entities\n",
      "INFO:minirag:Inserting 271 vectors to entities_name\n",
      "INFO:minirag:Inserting 134 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1786 nodes, 1027 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-6c184b01d5f088d78af9427b7f7f2db0 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-6c184b01d5f088d78af9427b7f7f2db0. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-6c184b01d5f088d78af9427b7f7f2db0 summarized: 5700 â†’ 357 chars (93.7% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 80 chunks, 435 entities(duplicated), 88 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 264 vectors to entities\n",
      "INFO:minirag:Inserting 264 vectors to entities\n",
      "INFO:minirag:Inserting 264 vectors to entities_name\n",
      "INFO:minirag:Inserting 83 vectors to relationships\n",
      "INFO:minirag:Writing graph with 1882 nodes, 1075 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-6744be8aacbb5dc28b515dfc31c967a9 in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-6744be8aacbb5dc28b515dfc31c967a9. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-6744be8aacbb5dc28b515dfc31c967a9 summarized: 1640 â†’ 353 chars (78.5% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 82 chunks, 441 entities(duplicated), 169 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 278 vectors to entities\n",
      "INFO:minirag:Inserting 278 vectors to entities\n",
      "INFO:minirag:Inserting 278 vectors to entities_name\n",
      "INFO:minirag:Inserting 167 vectors to relationships\n",
      "INFO:minirag:Writing graph with 2012 nodes, 1194 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-ddb0d87b57363b2cb71ea4828e32aa9a in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-ddb0d87b57363b2cb71ea4828e32aa9a. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-ddb0d87b57363b2cb71ea4828e32aa9a summarized: 1314 â†’ 364 chars (72.3% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 84 chunks, 474 entities(duplicated), 231 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 297 vectors to entities\n",
      "INFO:minirag:Inserting 297 vectors to entities\n",
      "INFO:minirag:Inserting 297 vectors to entities_name\n",
      "INFO:minirag:Inserting 222 vectors to relationships\n",
      "INFO:minirag:Writing graph with 2158 nodes, 1328 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-48961762ef4c121df3fa16dd2b43f4bf in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-48961762ef4c121df3fa16dd2b43f4bf. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-48961762ef4c121df3fa16dd2b43f4bf summarized: 557 â†’ 293 chars (47.4% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ´ Processed 85 chunks, 462 entities(duplicated), 136 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 291 vectors to entities\n",
      "INFO:minirag:Inserting 291 vectors to entities\n",
      "INFO:minirag:Inserting 291 vectors to entities_name\n",
      "INFO:minirag:Inserting 127 vectors to relationships\n",
      "INFO:minirag:Writing graph with 2260 nodes, 1388 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-1844cbf15e87a346bac192889cc96d4a in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-1844cbf15e87a346bac192889cc96d4a. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-1844cbf15e87a346bac192889cc96d4a summarized: 2132 â†’ 338 chars (84.1% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‡ Processed 88 chunks, 479 entities(duplicated), 150 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 291 vectors to entities\n",
      "INFO:minirag:Inserting 291 vectors to entities\n",
      "INFO:minirag:Inserting 291 vectors to entities_name\n",
      "INFO:minirag:Inserting 138 vectors to relationships\n",
      "INFO:minirag:Writing graph with 2374 nodes, 1484 edges\n",
      "INFO:minirag:Stored 1 new unique documents\n",
      "INFO:minirag:Number of batches to process: 1\n",
      "INFO:minirag:Processing document doc-9fb92cf5f5389391fb7dafa22c3afb7d in batch 1/1\n",
      "WARNING:minirag:Both BART and LexRank enabled for doc-9fb92cf5f5389391fb7dafa22c3afb7d. Applying LexRankâ†’BART pipeline\n",
      "INFO:minirag:  Step 1/2: LexRank extraction (ratio: 1)\n",
      "INFO:minirag:  Step 2/2: BART abstractive summarization on LexRank result (ratio: 1)\n",
      "INFO:minirag:Document doc-9fb92cf5f5389391fb7dafa22c3afb7d summarized: 2940 â†’ 342 chars (88.4% reduction)\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:Document processing pipeline completed\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 93 chunks, 497 entities(duplicated), 156 relations(duplicated)\r"
     ]
    }
   ],
   "source": [
    "import os, time, json, random, gc, asyncio\n",
    "from pathlib import Path\n",
    "import psutil, torch\n",
    "import minirag\n",
    "from minirag.llm import ollama\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\"\"\"\n",
    "Cell 1: Document Ingestion / Indexing Only\n",
    "-----------------------------------------\n",
    "Run this FIRST. It builds the MiniRAG index (vectors + KG) from source documents.\n",
    "No query / evaluation logic here.\n",
    "\"\"\"\n",
    "# ---------------- User Config ----------------\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\notebooks\\\\storage_bart_lexrank\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # set to None if no LLM available\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents():\n",
    "    global rag  # expose for cell 2\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing with ainsert() ...\")\n",
    "    t1 = time.perf_counter()\n",
    "    for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "        try:\n",
    "            await rag.ainsert(text, metadata=metadata, file_path=metadata.get(\"source\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)} docs in {dur:.2f}s ({len(texts)/dur:.2f} docs/s)\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Î” {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "# Clear the working directory if you want a fresh start (UNCOMMENT TO RESET)\n",
    "# import shutil\n",
    "# if os.path.exists(WORKING_DIR):\n",
    "#     print(f\"ðŸ”„ Clearing {WORKING_DIR} for fresh indexing...\")\n",
    "#     shutil.rmtree(WORKING_DIR)\n",
    "#     os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "#     print(f\"âœ“ Cleared. Ready for fresh indexing.\")\n",
    "\n",
    "await index_documents()\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Supports:\n",
    "#  - Loading LiHua-World QA pairs from query_set.csv\n",
    "#  - Evaluating answer quality with simple + lexical + semantic metrics\n",
    "#  - Optional CSV logging\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "# from minirag import QueryParam\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) â€“ not used now\n",
    "\n",
    "# Extra metric libs (lazy loads handled in compute_similarity)\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "QUERY_MODE = \"naive\"      # mini | light | naive | doc | meta | bm25\n",
    "TOP_K = 5\n",
    "MAX_Q = None             # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True       # toggle semantic similarity (slower)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# -------- Metrics Helpers --------\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "\n",
    "# lazy globals\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH_FN = SmoothingFunction().method1\n",
    "\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "_BERT_MODEL = None\n",
    "_ROUGE = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    \"\"\"\n",
    "    Returns the highest similarity score (float) between any sentence in `sentences` and `target`.\n",
    "    Methods: jaccard | levenshtein | rouge | bert | overlap | bleu\n",
    "    For rouge, n=1 or 2 selects rouge-1 or rouge-2 F.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens).intersection(set(tgt_set))\n",
    "            union = set(s_tokens).union(set(tgt_set))\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "\n",
    "    elif method == \"rouge\":\n",
    "        global _ROUGE\n",
    "        if _ROUGE is None:\n",
    "            _ROUGE = Rouge()\n",
    "        key = f\"rouge-{n}\"\n",
    "        for s in sentences:\n",
    "            r = _ROUGE.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "\n",
    "    elif method == \"bert\":\n",
    "        global _BERT_MODEL\n",
    "        if _BERT_MODEL is None:\n",
    "            _BERT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = _BERT_MODEL.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    \"\"\"Compute a bundle of similarity scores between answer and gold.\n",
    "\n",
    "    Returns keys:\n",
    "      jaccard, levenshtein, rouge1_f, rouge2_f, overlap, bleu, bert_cos (optional)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(answer)\n",
    "    jaccard = calculate_best_similarity(sentences, gold, method=\"jaccard\")\n",
    "    levenshtein = calculate_best_similarity(sentences, gold, method=\"levenshtein\")\n",
    "    rouge1_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=1)\n",
    "    rouge2_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=2)\n",
    "    overlap = calculate_best_similarity(sentences, gold, method=\"overlap\")\n",
    "    bleu = calculate_best_similarity(sentences, gold, method=\"bleu\")\n",
    "    bert_cos = calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None\n",
    "\n",
    "    result = {\n",
    "        'jaccard': jaccard,\n",
    "        'levenshtein': levenshtein,\n",
    "        'rouge1_f': rouge1_f,\n",
    "        'rouge2_f': rouge2_f,\n",
    "        'overlap': overlap,\n",
    "        'bleu': bleu,\n",
    "    }\n",
    "    if bert_cos is not None:\n",
    "        result['bert_cos'] = bert_cos\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(answer: str, gold: str) -> dict:\n",
    "    # Basic lexical metrics\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "\n",
    "    sim_bundle = compute_similarity(answer, gold, use_bert=USE_BERT_SIM)\n",
    "\n",
    "    return {\n",
    "        'exact': exact,\n",
    "        'substring': substring,\n",
    "        'token_recall': token_recall,\n",
    "        **sim_bundle,\n",
    "    }\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "\n",
    "# -------- Evaluation --------\n",
    "async def run_eval(mode, n):\n",
    "    qp = QueryParam(mode=mode, top_k=TOP_K)\n",
    "    rows = []\n",
    "    latencies = []\n",
    "\n",
    "    for i, (question, gold) in enumerate(tqdm(qa_pairs, total=len(qa_pairs), desc=f\"Eval-{mode}\", unit=\"q\"), start=1):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            answer = await rag.aquery(question, param=qp)\n",
    "        except TypeError:\n",
    "            answer = await rag.aquery(question)\n",
    "        latency = time.perf_counter() - t0\n",
    "        latencies.append(latency)\n",
    "\n",
    "        m = compute_metrics(answer, gold)\n",
    "        row = {\"question\": question, \"gold\": gold, \"answer\": answer, \"latency_s\": latency, **m}\n",
    "        rows.append(row)\n",
    "\n",
    "        if i <= 2:\n",
    "            # use tqdm.write to avoid breaking the progress bar formatting\n",
    "            tqdm.write(f\"Q{i}: {question[:80]}...\")\n",
    "            tqdm.write(\"Answer: \" + answer[:180].replace(\"\\n\", \" \"))\n",
    "            tqdm.write(\"Gold: \" + gold[:180])\n",
    "            # Format numeric (non-NaN) metrics to 3 decimals\n",
    "            fmt_metrics = {\n",
    "                k: (f\"{v:.3f}\" if isinstance(v, (int, float)) and not (isinstance(v, float) and math.isnan(v)) else v)\n",
    "                for k, v in m.items()\n",
    "            }\n",
    "            tqdm.write(f\"Metrics: {fmt_metrics} Latency: {latency*1000:.1f} ms\")\n",
    "            tqdm.write('-')\n",
    "    # Aggregates\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "\n",
    "    exact_rate = _avg('exact')\n",
    "    substr_rate = _avg('substring')\n",
    "    avg_token_recall = _avg('token_recall')\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95_lat = sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies) > 1 else latencies[0]\n",
    "\n",
    "    print(f\"\\nAggregate: exact={exact_rate:.2%} substring={substr_rate:.2%} token_recall={avg_token_recall:.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if mkey in rows[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms\")\n",
    "\n",
    "    os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "    OUTPUT_CSV = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "    # Optional CSV\n",
    "    if OUTPUT_CSV and rows:\n",
    "        write_header = not os.path.exists(OUTPUT_CSV)\n",
    "        with open(OUTPUT_CSV, 'a', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            if write_header: writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"Saved results to {OUTPUT_CSV}\")\n",
    "    return rows\n",
    "\n",
    "# Run evaluation\n",
    "eval_results1 = await run_eval(\"light\", 5)\n",
    "eval_results2 = await run_eval(\"mini\", 5)\n",
    "eval_results3 = await run_eval(\"naive\", 5)\n",
    "eval_results4 = await run_eval(\"bypass\", 5)\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
