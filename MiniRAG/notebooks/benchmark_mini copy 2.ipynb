{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device: cuda\n",
      "Loading embedding tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Logger initialized for working directory: C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\notebooks\\\\storage_regex\n",
      "INFO:minirag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:minirag:Load KV llm_response_cache with 0 data\n",
      "INFO:minirag:Load KV full_docs with 0 data\n",
      "INFO:minirag:Load KV text_chunks with 0 data\n",
      "INFO:minirag:Loaded graph from C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\notebooks\\\\storage_regex\\graph_chunk_entity_relation.graphml with 635 nodes, 3553 edges\n",
      "INFO:nano-vectordb:Load (635, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_regex\\\\vdb_entities.json'} 635 data\n",
      "INFO:nano-vectordb:Load (635, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_regex\\\\vdb_entities_name.json'} 635 data\n",
      "INFO:nano-vectordb:Load (3553, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_regex\\\\vdb_relationships.json'} 3553 data\n",
      "INFO:nano-vectordb:Load (0, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': 'C:\\\\\\\\Users\\\\\\\\Francisco Azeredo\\\\\\\\OneDrive\\\\\\\\Documents\\\\\\\\tecnico\\\\\\\\5 ano\\\\\\\\tese\\\\\\\\CÃ³digo\\\\\\\\MiniRAG\\\\\\\\notebooks\\\\\\\\storage_regex\\\\vdb_chunks.json'} 0 data\n",
      "INFO:minirag:Loaded document status storage with 77 records\n",
      "INFO:minirag.summarization.bart_summarizer:Loaded BART model: facebook/bart-large-cnn on cuda\n",
      "INFO:minirag:BART summarizer initialized for entity description summarization\n",
      "INFO:minirag:LexRank summarizer initialized for entity extraction (ratio: 0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Loads embedding model, builds embedding_func, and instantiates a MiniRAG object.\n",
    "# Does NOT ingest documents. Use the next cell to index.\n",
    "\n",
    "import os, torch, sys\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from minirag.llm.hf import hf_embed\n",
    "from minirag.utils import EmbeddingFunc\n",
    "from minirag.llm import ollama\n",
    "from minirag.llm.openai import openai_complete\n",
    "from minirag import MiniRAG\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment. Set it before running this cell.\")\n",
    "\n",
    "sys.path.append(r'C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag')\n",
    "\n",
    "# Core configuration (shared by later cells)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\CÃ³digo\\\\MiniRAG\\\\notebooks\\\\storage_regex\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # set to None if no local Ollama model\n",
    "LOG_LEVEL = \"INFO\"  # Changed from \"CRITICAL\" to see initialization messages\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Init device:\", device)\n",
    "\n",
    "print(\"Loading embedding tokenizer/model...\")\n",
    "_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "_embed_model = AutoModel.from_pretrained(EMBEDDING_MODEL).to(device)\n",
    "_embed_model.eval()\n",
    "\n",
    "async def _embed_batch(texts: list[str]):\n",
    "    return await hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model)\n",
    "\n",
    "async def _embed_dispatch(input_text):\n",
    "    if isinstance(input_text, str):\n",
    "        return (await _embed_batch([input_text]))[0]\n",
    "\n",
    "\n",
    "        \n",
    "    if isinstance(input_text, (list, tuple)) and all(isinstance(t, str) for t in input_text):\n",
    "        return await _embed_batch(list(input_text))\n",
    "    raise TypeError(f\"Unsupported input type for embedding_func: {type(input_text)}\")\n",
    "\n",
    "_embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=_embed_model.config.hidden_size,\n",
    "    max_token_size=_tokenizer.model_max_length,\n",
    "    func = lambda texts: hf_embed(texts, tokenizer=_tokenizer, embed_model=_embed_model),\n",
    ")\n",
    "rag = MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama.ollama_model_complete if LLM_MODEL_NAME else None,\n",
    "    llm_model_name=LLM_MODEL_NAME,\n",
    "    embedding_func=_embedding_func,\n",
    "    log_level=LOG_LEVEL,\n",
    "    suppress_httpx_logging=True,\n",
    "    summarize_before_chunking=True,\n",
    "    document_summary_ratio=0.3,\n",
    "    use_bart_entity_extraction=True,\n",
    "    key_sentences_lexrank=True,\n",
    "    lexrank_ratio=0.3,\n",
    ")\n",
    "print(\"RAG initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 442 docs in 0.06s\n",
      "Start RSS: 1162.22 MB\n",
      "Indexing with ainsert() ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd09027b96ce44aebdf165807377503e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:No new unique documents were found.\n",
      "INFO:minirag:No documents to process\n",
      "INFO:minirag:Performing entity extraction on newly processed chunks\n",
      "INFO:minirag:Using entity extraction with BART description summarization\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    114\u001b[39m     gc.collect(); end_mem = memory_mb()\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end_mem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnd RSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB (Î” \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_mem\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m index_documents()\n\u001b[32m    118\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIndexing complete. Proceed to Cell 2 for querying & evaluation.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mindex_documents\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text, metadata \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(texts, metas), desc=\u001b[33m\"\u001b[39m\u001b[33mIndexing\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(texts)):\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m rag.ainsert(text, metadata=metadata, file_path=metadata.get(\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    111\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata.get(\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag\\minirag.py:461\u001b[39m, in \u001b[36mMiniRAG.ainsert\u001b[39m\u001b[34m(self, input, split_by_character, split_by_character_only, ids, file_path, metadata)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    460\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing entity extraction\u001b[39m\u001b[33m\"\u001b[39m + (\u001b[33m\"\u001b[39m\u001b[33m with BART description summarization\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.summarizer \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m extract_entities(     \n\u001b[32m    462\u001b[39m             inserting_chunks,\n\u001b[32m    463\u001b[39m             knowledge_graph_inst=\u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m    464\u001b[39m             entity_vdb=\u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m    465\u001b[39m             entity_name_vdb=\u001b[38;5;28mself\u001b[39m.entity_name_vdb,\n\u001b[32m    466\u001b[39m             relationships_vdb=\u001b[38;5;28mself\u001b[39m.relationships_vdb,\n\u001b[32m    467\u001b[39m             global_config=asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    468\u001b[39m         )\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# Mark BM25 index dirty after any insertion\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_bm25_index_state\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag\\operate.py:510\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(chunks, knowledge_graph_inst, entity_vdb, entity_name_vdb, relationships_vdb, global_config)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    504\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow_ticks\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malready_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malready_entities\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entities(duplicated), \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malready_relations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m relations(duplicated)\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    505\u001b[39m         end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    506\u001b[39m         flush=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    507\u001b[39m     )\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(maybe_nodes), \u001b[38;5;28mdict\u001b[39m(maybe_edges)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    511\u001b[39m     *[_process_single_content(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ordered_chunks]\n\u001b[32m    512\u001b[39m )\n\u001b[32m    513\u001b[39m maybe_nodes = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    514\u001b[39m maybe_edges = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag\\operate.py:450\u001b[39m, in \u001b[36mextract_entities.<locals>._process_single_content\u001b[39m\u001b[34m(chunk_key_dp)\u001b[39m\n\u001b[32m    448\u001b[39m content = chunk_dp[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    449\u001b[39m hint_prompt = entity_extract_prompt.format(**context_base, input_text=content)\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m final_result = \u001b[38;5;28;01mawait\u001b[39;00m use_llm_func(hint_prompt)\n\u001b[32m    452\u001b[39m history = pack_user_ass_to_openai_messages(hint_prompt, final_result)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m now_glean_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(entity_extract_max_gleaning):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\minirag\\utils.py:113\u001b[39m, in \u001b[36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mnonlocal\u001b[39;00m __current_size\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m __current_size >= max_size:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(waitting_time)\n\u001b[32m    114\u001b[39m __current_size += \u001b[32m1\u001b[39m\n\u001b[32m    115\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\asyncio\\tasks.py:665\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    661\u001b[39m h = loop.call_later(delay,\n\u001b[32m    662\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    663\u001b[39m                     future, result)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Clear the working directory if you want a fresh start (UNCOMMENT TO RESET)\n",
    "import shutil\n",
    "if os.path.exists(WORKING_DIR):\n",
    "    print(f\"ðŸ”„ Clearing {WORKING_DIR} for fresh indexing...\")\n",
    "    shutil.rmtree(WORKING_DIR)\n",
    "    os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "    print(f\"âœ“ Cleared. Ready for fresh indexing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29269f",
   "metadata": {},
   "source": [
    "## Why `apipeline_process_enqueue_documents()` is being skipped\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "When you re-run the indexing cell, documents are skipped with message: `\"No new unique documents were found.\"` and `\"No documents to process\"`\n",
    "\n",
    "**Root Cause:**\n",
    "\n",
    "The MiniRAG indexing pipeline has this flow:\n",
    "\n",
    "```\n",
    "1. apipeline_enqueue_documents()\n",
    "   â”œâ”€ Mark docs as PENDING\n",
    "   â””â”€ Filter out duplicates (docs that already exist in storage)\n",
    "   \n",
    "2. apipeline_process_enqueue_documents()\n",
    "   â”œâ”€ Process PENDING/PROCESSING/FAILED docs\n",
    "   â”œâ”€ Chunk documents\n",
    "   â”œâ”€ Mark as PROCESSED\n",
    "   â””â”€ (But if this fails, docs stay PROCESSED with NO chunks!)\n",
    "\n",
    "3. extract_entities() \n",
    "   â”œâ”€ Extract KG entities from chunks\n",
    "   â””â”€ (This is where LLM timeout occurs â†’ CancelledError)\n",
    "```\n",
    "\n",
    "**Why you see \"No documents to process\":**\n",
    "\n",
    "1. **First insertion attempt:**\n",
    "   - Documents marked PENDING âœ“\n",
    "   - `apipeline_process_enqueue_documents()` runs âœ“\n",
    "   - Documents marked PROCESSED âœ“\n",
    "   - `extract_entities()` starts... **LLM timeout/cancel** âœ—\n",
    "   - Documents remain PROCESSED (but have NO chunks)\n",
    "\n",
    "2. **Next iteration** (or cell re-run):\n",
    "   - New docs â†’ `filter_keys()` removes them (already exist)\n",
    "   - \"No new unique documents were found\"\n",
    "   - `apipeline_process_enqueue_documents()` skipped\n",
    "   - **Chunking pipeline never runs!**\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Clear the working directory to start fresh:\n",
    "```python\n",
    "shutil.rmtree(WORKING_DIR)\n",
    "```\n",
    "\n",
    "This is now done automatically in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Supports:\n",
    "#  - Loading LiHua-World QA pairs from query_set.csv\n",
    "#  - Evaluating answer quality with simple + lexical + semantic metrics\n",
    "#  - Optional CSV logging\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "# from minirag import QueryParam\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) â€“ not used now\n",
    "\n",
    "# Extra metric libs (lazy loads handled in compute_similarity)\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\CÃ³digo\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "QUERY_MODE = \"naive\"      # mini | light | naive | doc | meta | bm25\n",
    "TOP_K = 5\n",
    "MAX_Q = None             # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True       # toggle semantic similarity (slower)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# -------- Metrics Helpers --------\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "\n",
    "# lazy globals\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH_FN = SmoothingFunction().method1\n",
    "\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "_BERT_MODEL = None\n",
    "_ROUGE = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    \"\"\"\n",
    "    Returns the highest similarity score (float) between any sentence in `sentences` and `target`.\n",
    "    Methods: jaccard | levenshtein | rouge | bert | overlap | bleu\n",
    "    For rouge, n=1 or 2 selects rouge-1 or rouge-2 F.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens).intersection(set(tgt_set))\n",
    "            union = set(s_tokens).union(set(tgt_set))\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "\n",
    "    elif method == \"rouge\":\n",
    "        global _ROUGE\n",
    "        if _ROUGE is None:\n",
    "            _ROUGE = Rouge()\n",
    "        key = f\"rouge-{n}\"\n",
    "        for s in sentences:\n",
    "            r = _ROUGE.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "\n",
    "    elif method == \"bert\":\n",
    "        global _BERT_MODEL\n",
    "        if _BERT_MODEL is None:\n",
    "            _BERT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = _BERT_MODEL.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    \"\"\"Compute a bundle of similarity scores between answer and gold.\n",
    "\n",
    "    Returns keys:\n",
    "      jaccard, levenshtein, rouge1_f, rouge2_f, overlap, bleu, bert_cos (optional)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(answer)\n",
    "    jaccard = calculate_best_similarity(sentences, gold, method=\"jaccard\")\n",
    "    levenshtein = calculate_best_similarity(sentences, gold, method=\"levenshtein\")\n",
    "    rouge1_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=1)\n",
    "    rouge2_f = calculate_best_similarity(sentences, gold, method=\"rouge\", n=2)\n",
    "    overlap = calculate_best_similarity(sentences, gold, method=\"overlap\")\n",
    "    bleu = calculate_best_similarity(sentences, gold, method=\"bleu\")\n",
    "    bert_cos = calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None\n",
    "\n",
    "    result = {\n",
    "        'jaccard': jaccard,\n",
    "        'levenshtein': levenshtein,\n",
    "        'rouge1_f': rouge1_f,\n",
    "        'rouge2_f': rouge2_f,\n",
    "        'overlap': overlap,\n",
    "        'bleu': bleu,\n",
    "    }\n",
    "    if bert_cos is not None:\n",
    "        result['bert_cos'] = bert_cos\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(answer: str, gold: str) -> dict:\n",
    "    # Basic lexical metrics\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "\n",
    "    sim_bundle = compute_similarity(answer, gold, use_bert=USE_BERT_SIM)\n",
    "\n",
    "    return {\n",
    "        'exact': exact,\n",
    "        'substring': substring,\n",
    "        'token_recall': token_recall,\n",
    "        **sim_bundle,\n",
    "    }\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "assert 'rag' in globals(), \"rag not found. Run Cell 1 first.\"\n",
    "\n",
    "# -------- Evaluation --------\n",
    "async def run_eval(mode, n):\n",
    "    qp = QueryParam(mode=mode, top_k=TOP_K)\n",
    "    rows = []\n",
    "    latencies = []\n",
    "\n",
    "    for i, (question, gold) in enumerate(tqdm(qa_pairs, total=len(qa_pairs), desc=f\"Eval-{mode}\", unit=\"q\"), start=1):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            answer = await rag.aquery(question, param=qp)\n",
    "        except TypeError:\n",
    "            answer = await rag.aquery(question)\n",
    "        latency = time.perf_counter() - t0\n",
    "        latencies.append(latency)\n",
    "\n",
    "        m = compute_metrics(answer, gold)\n",
    "        row = {\"question\": question, \"gold\": gold, \"answer\": answer, \"latency_s\": latency, **m}\n",
    "        rows.append(row)\n",
    "\n",
    "        if i <= 2:\n",
    "            # use tqdm.write to avoid breaking the progress bar formatting\n",
    "            tqdm.write(f\"Q{i}: {question[:80]}...\")\n",
    "            tqdm.write(\"Answer: \" + answer[:180].replace(\"\\n\", \" \"))\n",
    "            tqdm.write(\"Gold: \" + gold[:180])\n",
    "            # Format numeric (non-NaN) metrics to 3 decimals\n",
    "            fmt_metrics = {\n",
    "                k: (f\"{v:.3f}\" if isinstance(v, (int, float)) and not (isinstance(v, float) and math.isnan(v)) else v)\n",
    "                for k, v in m.items()\n",
    "            }\n",
    "            tqdm.write(f\"Metrics: {fmt_metrics} Latency: {latency*1000:.1f} ms\")\n",
    "            tqdm.write('-')\n",
    "    # Aggregates\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "\n",
    "    exact_rate = _avg('exact')\n",
    "    substr_rate = _avg('substring')\n",
    "    avg_token_recall = _avg('token_recall')\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95_lat = sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies) > 1 else latencies[0]\n",
    "\n",
    "    print(f\"\\nAggregate: exact={exact_rate:.2%} substring={substr_rate:.2%} token_recall={avg_token_recall:.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if mkey in rows[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms\")\n",
    "\n",
    "    os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "    OUTPUT_CSV = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "    # Optional CSV\n",
    "    if OUTPUT_CSV and rows:\n",
    "        write_header = not os.path.exists(OUTPUT_CSV)\n",
    "        with open(OUTPUT_CSV, 'a', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            if write_header: writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        print(f\"Saved results to {OUTPUT_CSV}\")\n",
    "    return rows\n",
    "\n",
    "# Run evaluation\n",
    "eval_results1 = await run_eval(\"light\", 5)\n",
    "eval_results2 = await run_eval(\"mini\", 5)\n",
    "eval_results3 = await run_eval(\"naive\", 5)\n",
    "eval_results4 = await run_eval(\"bypass\", 5)\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
